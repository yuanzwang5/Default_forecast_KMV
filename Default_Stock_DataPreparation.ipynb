{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662da8ff-5b79-4851-8d15-2cf503b73d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import requests\n",
    "import matplotlib as mpl\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "ts.set_token('4264bdda6fbb66c87e086cfadaecdc887566b8c689f000965db52f3e')\n",
    "pro = ts.pro_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e8150",
   "metadata": {},
   "source": [
    "# Every year's trading days number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24710cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tradeday = pro.trade_cal(exchange='', start_date='20110101', end_date='20201231')\n",
    "df_tradeday.head()\n",
    "df_count=df_tradeday.groupby(df_tradeday['cal_date'].apply(lambda x:x[:4])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4979e318-e69b-4615-bd03-b9085d930921",
   "metadata": {},
   "source": [
    "# Risk-free Rate (Shibor in %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c796020-a796-4f76-9233-4d227c5a6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Default_Data/')\n",
    "os.chdir('Shibor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d98f42-e7e6-4644-b6e2-e17a3cec3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_last = [] # Every year last date's risk-free rate\n",
    "rf_mean = [] # Every year's mean risk-free rate\n",
    "year= []\n",
    "for file in os.listdir():\n",
    "    print(file)\n",
    "    data = pd.read_excel(file)\n",
    "    year.append(file[7:11])\n",
    "    rf_last.append(data.iloc[data.shape[0]-1,data.shape[1]-1])\n",
    "    rf_mean.append(np.mean(data['1Y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37763e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14824a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd81dce",
   "metadata": {},
   "source": [
    "# Match Stock ts_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ffa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bond issuers' name from the excel file\n",
    "sheet = pd.read_excel('Bond_Issuer_ListCompornot.xlsx')\n",
    "manually = pd.read_excel('manually.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec36923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information of list companies in SH and SZ market\n",
    "basic_SHSZ = pro.stock_basic(exchange='', list_status='L', fields='ts_code,fullname')\n",
    "# Basic information of list companies in HK market\n",
    "basic_HK = pro.hk_basic(list_status='L', fields='ts_code,fullname')\n",
    "# Merge them together\n",
    "frames = [basic_SHSZ, basic_HK, manually]\n",
    "basic = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db316e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stock codes\n",
    "stockCode = sheet.merge(basic, on='fullname',how='left')\n",
    "stockCode = pd.DataFrame(stockCode, columns=['fullname','ts_code'])\n",
    "stockCode = stockCode.dropna(subset=['ts_code'])\n",
    "stockCode.to_excel('stock_list.xlsx', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567356c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnotin(a,b):\n",
    "    if a in b:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockCode_SHSZ = stockCode[stockCode['ts_code'].map(lambda x:isnotin('.HK',x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockCode_SHSZ = stockCode_SHSZ['ts_code'].tolist()\n",
    "stockCode_SHSZ = list(set(stockCode_SHSZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac340f6",
   "metadata": {},
   "source": [
    "# 短期负债 长期负债 企业债券面值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSMAR debt data\n",
    "combas = pd.read_excel('FS_Combas.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "combasData = combas.drop(combas[combas['Typrep'] == 'B'].index)\n",
    "del combasData['Typrep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6dc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combasData.columns = ['Stkcd','end_date','current_debt','long_term_debt','non_current_debt','total_debt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43089b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropList = []\n",
    "for i in list(set(combasData['end_date'].tolist())):\n",
    "    if i < '2011-12-31' or (i.endswith('12-31') == False):\n",
    "        dropList.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dropList:\n",
    "    combasData = combasData.drop(combasData[combasData['end_date'] == i].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3102ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Default_Data/')\n",
    "os.chdir('debt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60056641",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Short-term debt, Long-term debt, Face value from Tushare\n",
    "for i in stockCode_SHSZ:\n",
    "    print(i)\n",
    "    # Debt\n",
    "    debtData = pro.balancesheet(ts_code=i, start_date='20110101', end_date='20210630', fields='end_date,total_cur_liab,total_ncl,total_liab')\n",
    "    debtNew=debtData[debtData['end_date'].apply(lambda x:x[4:])=='1231']\n",
    "    debt=debtNew.drop_duplicates()\n",
    "    #debt.sort_values('end_date',inplace=True)\n",
    "    debt.to_excel(i+'_debt.xlsx',index=False,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ec69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1ea80",
   "metadata": {},
   "source": [
    "# 每股净资产"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSMAR bps data\n",
    "combas = pd.read_excel('FS_Combas.xlsx')\n",
    "combasData = combas.drop(combas[combas['Typrep'] == 'B'].index)\n",
    "del combasData['Typrep']\n",
    "combasData.columns = ['Stkcd','end_date','current_debt','long_term_debt','non_current_debt','total_debt']\n",
    "dropList = []\n",
    "for i in list(set(combasData['end_date'].tolist())):\n",
    "    if i < '2011-12-31' or (i.endswith('12-31') == False):\n",
    "        dropList.append(i)\n",
    "for i in dropList:\n",
    "    combasData = combasData.drop(combasData[combasData['end_date'] == i].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48732621",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Default_Data/')\n",
    "os.chdir('bps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9f8ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in stockCode_SHSZ:\n",
    "    print(i)\n",
    "    bpsData = pro.fina_indicator(ts_code=i, start_date='20110101', end_date='20210630', fields='ts_code,end_date,bps')\n",
    "    d = dict(zip(bpsData['end_date'],bpsData['bps']))\n",
    "    bpsData = []\n",
    "    for j in d.keys():\n",
    "        if j.endswith('1231') == True:\n",
    "            bpsData.append([j,d[j]])\n",
    "    bpsData = sorted(bpsData, key=lambda x:x[0], reverse=False)\n",
    "    bps = pd.DataFrame(bpsData)\n",
    "    bps.columns = ['date','bps']\n",
    "    bps.to_excel(i+'_bps.xlsx',index=False,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86cb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b18e6e",
   "metadata": {},
   "source": [
    "# 流通股与非流通股"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b227147",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Default_Data/')\n",
    "capchg = pd.read_csv('TRD_Capchg.csv')\n",
    "os.chdir('../')\n",
    "capchgData = pd.DataFrame(capchg, columns=['Stkcd','Shrchgdt','Nshrttl','Nshra'])\n",
    "capchgData['Stkcd'] = pd.DataFrame([str(x).zfill(6) for x in capchgData['Stkcd'].tolist()])\n",
    "stkcd = capchgData['Stkcd'].tolist()\n",
    "for i in range(len(stkcd)):\n",
    "    if stkcd[i][0] == '0' or stkcd[i][0] == '3':\n",
    "        stkcd[i] = stkcd[i] + '.SZ'\n",
    "    else:\n",
    "        stkcd[i] = stkcd[i] + '.SH'\n",
    "capchgData['Stkcd'] = pd.DataFrame(stkcd)\n",
    "capchgData['Shrchgdt'] = pd.DataFrame([x.replace('-','') for x in capchgData['Shrchgdt'].tolist()])\n",
    "lastDate = [x+'1231' for x in year]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814aa71",
   "metadata": {},
   "source": [
    "# Weekly Stock Close Price 使用前复权后的周收盘价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Default_Data/weekly_close/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e07970",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in stockCode_SHSZ:\n",
    "    print(i)\n",
    "    closeData = ts.pro_bar(ts_code=i, freq='W', adj='qfq', start_date='20110101', end_date='20210101')\n",
    "    closeData['ln_return'] = [math.log(closeData['close'][i]/closeData['pre_close'][i]) for i in range(len(closeData))]\n",
    "    closeData['weekly_return'] = [(closeData['close'][i] - closeData['pre_close'][i])/closeData['pre_close'][i] for i in range(len(closeData))]\n",
    "    closeData['square_ln'] = [pow(i,2) for i in closeData['ln_return'].tolist()]\n",
    "    closeData['square_weekly'] = [pow(i,2) for i in closeData['weekly_return'].tolist()]\n",
    "    closeData.to_excel(i+'_weekly.xlsx', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4109e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf64bcb",
   "metadata": {},
   "source": [
    "# Summary Codes to calculate the default probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "i='000008.SZ'\n",
    "debtData = pro.balancesheet(ts_code=i, start_date='20110101', end_date='20210630', fields='end_date,total_cur_liab,total_ncl,total_liab')\n",
    "debtNew=debtData[debtData['end_date'].apply(lambda x:x[4:])=='1231']\n",
    "debt=debtNew.drop_duplicates()\n",
    "shortTerm = debt['total_cur_liab'].tolist()\n",
    "longTerm = debt['total_ncl'].tolist()\n",
    "nan = longTerm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2116de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate sigmaE\n",
    "def sigmaE(n, squaresum, sumsquare):\n",
    "    if n <= 1:\n",
    "        sigmae = nan\n",
    "    else:\n",
    "        if (squaresum/(n-1)) - 1/(n*(n-1)*pow(sumsquare,2)) <= 0:\n",
    "            sigmae = nan\n",
    "        else:\n",
    "            nominator = math.sqrt((squaresum/(n-1)) - 1/(n*(n-1)*pow(sumsquare,2)))\n",
    "            denominator = math.sqrt(1/n)\n",
    "            sigmae = nominator/denominator\n",
    "    return sigmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2896384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import package\n",
    "from scipy.optimize import fsolve\n",
    "import math\n",
    "from scipy import stats\n",
    "#def exp sqrt\n",
    "def st(x):\n",
    "    x=float(x)\n",
    "    return math.exp(-(x**2)/2)\n",
    "#def normal distribution\n",
    "#def N(x):\n",
    "#    from scipy import integrate\n",
    "#    nx,err=integrate.quad(st,-100,x)\n",
    "#    nx=nx/math.sqrt(2*math.pi)\n",
    "#    return nx\n",
    "\n",
    "def N(x):\n",
    "    nx=stats.norm.cdf(x)\n",
    "    return nx\n",
    "\n",
    "#second function\n",
    "\n",
    "#def calculate the V and ThetaV\n",
    "def f(Equity,SigmaE,D,r,t=1):\n",
    "    Equity=float(Equity)\n",
    "    D=float(D)\n",
    "    r=float(r)\n",
    "    if Equity==0 or SigmaE==0 or D==0:\n",
    "        result=[0,0]\n",
    "        return result\n",
    "    else:\n",
    "        def ff(x):\n",
    "\n",
    "            x0=float(x[0])\n",
    "            x1=float(x[1])\n",
    "            d1=(math.log(abs(x0))-math.log(D)+(r+0.5*(x1**2))*t)/(x1*math.sqrt(t))\n",
    "            d2=d1-x1*math.sqrt(t)\n",
    "            return[\n",
    "                Equity-x0*N(d1)+D*math.exp(-r*t)*N(d2),\n",
    "                SigmaE-x0*x1*(N(d1)/Equity)\n",
    "            ]\n",
    "        result=fsolve(ff,[Equity,SigmaE])\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(141276427,0.2893,1.25e8,0.0225,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5c193",
   "metadata": {},
   "source": [
    "# Tushare and CSMAR ln and weekly return with rf_last (Don't use anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Default_Data/output_ln_return/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2b882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data from Tushare\n",
    "togetherFinal = []\n",
    "togetherWeeklyFinal = []\n",
    "for i in stockCode_SHSZ:\n",
    "    print(i)\n",
    "    # Debt\n",
    "    debtData = pro.balancesheet(ts_code=i, start_date='20110101', end_date='20210630', fields='end_date,total_cur_liab,total_ncl,total_liab')\n",
    "    debtNew=debtData[debtData['end_date'].apply(lambda x:x[4:])=='1231']\n",
    "    debt=debtNew.drop_duplicates()\n",
    "    shortTerm = debt['total_cur_liab'].tolist()\n",
    "    longTerm = debt['total_ncl'].tolist()\n",
    "    totalDebt = debt['total_liab'].tolist()\n",
    "    endDate = debt['end_date'].tolist()\n",
    "    \n",
    "    # Calculate three kinds of DP\n",
    "    dp = [] # DP\n",
    "    dp_50 = [] # DP50\n",
    "    dp_75 = [] # DP75\n",
    "    d_facevalue = [] # D\n",
    "    for y in year:\n",
    "        for e in endDate:\n",
    "            if e.startswith(y) == True:\n",
    "                current = shortTerm[endDate.index(e)]\n",
    "                long = longTerm[endDate.index(e)]\n",
    "                if (current == None) or (long == None):\n",
    "                    dp.append(nan)\n",
    "                    dp_50.append(nan)\n",
    "                    dp_75.append(nan)\n",
    "                else:\n",
    "                    dp.append(current)\n",
    "                    dp_50.append(current+0.5*long)\n",
    "                    dp_75.append(current+0.75*long)\n",
    "                d_facevalue.append(totalDebt[endDate.index(e)])\n",
    "            else:\n",
    "                pass\n",
    "    dpFrame = pd.DataFrame([lastDate,dp,dp_50,dp_75,d_facevalue]).T\n",
    "    dpFrame.columns = ['end_date','DP','DP50','DP75','D']\n",
    "    \n",
    "    \n",
    "    # BPS\n",
    "    bpsData = pro.fina_indicator(ts_code=i, start_date='20110101', end_date='20210630', fields='ts_code,end_date,bps')\n",
    "    d = dict(zip(bpsData['end_date'],bpsData['bps']))\n",
    "    bpsData = []\n",
    "    for j in d.keys():\n",
    "        if j.endswith('1231') == True:\n",
    "            bpsData.append([j,d[j]])\n",
    "    bpsData = sorted(bpsData, key=lambda x:x[0], reverse=False)\n",
    "    bps = pd.DataFrame(bpsData)\n",
    "    bps.columns = ['end_date','bps']\n",
    "    bpsData = bps['bps'].tolist()\n",
    "\n",
    "    # Weekly Stock Return\n",
    "    closeData = ts.pro_bar(ts_code=i, freq='W', adj='qfq', start_date='20110101', end_date='20210101')\n",
    "    #closeData = pro.weekly(ts_code=i, start_date='20110101', end_date='20210101', fields='trade_date,close,pre_close')\n",
    "    if type(closeData) == pd.DataFrame:\n",
    "        closeData['ln_return'] = [math.log(closeData['close'][i]/closeData['pre_close'][i]) for i in range(len(closeData))]\n",
    "        closeData['weekly_return'] = [(closeData['close'][i] - closeData['pre_close'][i])/closeData['pre_close'][i] for i in range(len(closeData))]\n",
    "        closeData['square_ln'] = [pow(i,2) for i in closeData['ln_return'].tolist()]\n",
    "        closeData['square_weekly'] = [pow(i,2) for i in closeData['weekly_return'].tolist()]\n",
    "        closePrice = closeData['close'].tolist()\n",
    "        lnList = closeData['ln_return'].tolist()\n",
    "        weeklyList = closeData['weekly_return'].tolist()    \n",
    "        squareLnList = closeData['square_ln'].tolist()\n",
    "        squareWeeklyList = closeData['square_weekly'].tolist()\n",
    "        tradeDate = closeData['trade_date'].tolist()\n",
    "    else:\n",
    "        pass\n",
    "  \n",
    "    \n",
    "    # Start to calculate KMV\n",
    "    sum_ln = []\n",
    "    sum_weekly = []\n",
    "    square_sum_ln = []\n",
    "    square_sum_weekly = []\n",
    "    mean_close = []\n",
    "    nList = []\n",
    "    for j in year:\n",
    "        #print(j)\n",
    "        sumln = []\n",
    "        sumweekly = []\n",
    "        squaresumln = []\n",
    "        squaresumweekly = []\n",
    "        meanClosePrice = []\n",
    "        for k in tradeDate:\n",
    "            if k.startswith(j) == True:\n",
    "                sumln.append(lnList[tradeDate.index(k)])\n",
    "                sumweekly.append(weeklyList[tradeDate.index(k)])\n",
    "                squaresumln.append(squareLnList[tradeDate.index(k)])\n",
    "                squaresumweekly.append(squareWeeklyList[tradeDate.index(k)])\n",
    "                meanClosePrice.append(closePrice[tradeDate.index(k)])\n",
    "            else:\n",
    "                pass\n",
    "        sum_ln.append(np.sum(sumln))\n",
    "        sum_weekly.append(np.sum(sumweekly))        \n",
    "        square_sum_ln.append(np.sum(squaresumln))\n",
    "        square_sum_weekly.append(np.sum(squaresumweekly))\n",
    "        if len(meanClosePrice) == 0:\n",
    "            mean_close.append(0)\n",
    "        else:\n",
    "            mean_close.append(np.mean(meanClosePrice))\n",
    "        nList.append(len(sumln))\n",
    "    print('Done')\n",
    "    weeklyReturn = pd.DataFrame([lastDate, sum_ln, sum_weekly, square_sum_ln, square_sum_weekly, mean_close, nList]).T\n",
    "    weeklyReturn.columns = ['end_date','ln_return','weekly_return','square_ln_return','square_weekly_return','mean_close','numOfWeeks']\n",
    "    \n",
    "    # Calculate E\n",
    "    # First, get the number of tradable shares and non-tradable shares\n",
    "    shareData = capchgData.loc[capchgData['Stkcd'] == i]\n",
    "    tradeShare = []\n",
    "    nonTradeShare = []\n",
    "    for l in lastDate:\n",
    "        if l <= shareData['Shrchgdt'].tolist()[0]:\n",
    "            total = 0\n",
    "            trade = 0\n",
    "        else:\n",
    "            total = shareData.loc[shareData['Shrchgdt'] <= l]['Nshrttl'].tolist()[-1]\n",
    "            trade = shareData.loc[shareData['Shrchgdt'] <= l]['Nshra'].tolist()[-1]\n",
    "        nonTrade = total - trade\n",
    "        tradeShare.append(trade)\n",
    "        nonTradeShare.append(nonTrade)\n",
    "    # Then calculate E\n",
    "    equity = [] # E\n",
    "    for r in range(len(bps)):\n",
    "        tradeEquity = mean_close[r] * tradeShare[r]\n",
    "        nonTradeEquity = bpsData[r] * nonTradeShare[r]\n",
    "        equityMktVal = tradeEquity + nonTradeEquity\n",
    "        equity.append(equityMktVal)\n",
    "        \n",
    "    # Use ln_Return to calculate sigmaE\n",
    "    sigma_E_ln = []\n",
    "    for ye in range(len(year)):\n",
    "        sigma_E_ln.append(sigmaE(nList[ye], square_sum_ln[ye], sum_ln[ye]))\n",
    "    eAndSigmaELn = pd.DataFrame([lastDate,equity,sigma_E_ln,rf_last]).T\n",
    "    eAndSigmaELn.columns = ['end_date','E','sigmaE','r']\n",
    "    \n",
    "    totalDf = dpFrame.merge(debt, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(bps, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(weeklyReturn, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(eAndSigmaELn, on='end_date',how='left')\n",
    "    finalDf = totalDf.dropna(axis=0,how='any')\n",
    "    \n",
    "\n",
    "    output = []\n",
    "    longth = finalDf.shape[0]\n",
    "    compcode = []\n",
    "    for ye in range(longth):\n",
    "        compcode.append(i)\n",
    "        Equity = finalDf['E'].tolist()[ye]\n",
    "        SigmaE = finalDf['sigmaE'].tolist()[ye]\n",
    "        D = finalDf['D'].tolist()[ye]\n",
    "        r = finalDf['r'].tolist()[ye]/100\n",
    "        medium =f(Equity,SigmaE,D,r,1)\n",
    "        if medium[0] == 0 or medium[1] == 0:\n",
    "            output.append('-')\n",
    "        else:\n",
    "            dd = (medium[0] - finalDf['DP50'].tolist()[ye])/(medium[0]*medium[1])\n",
    "            output.append(1-N(dd))\n",
    "    outputDf = pd.DataFrame([finalDf['end_date'].tolist(),output]).T\n",
    "    outputDf.columns = ['end_date','default_probability']\n",
    "    outputDf.to_excel(i+'.xlsx', index=False, encoding='utf_8_sig')\n",
    "    \n",
    "    if len(compcode) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        togetherOutput = pd.DataFrame([compcode, finalDf['end_date'].tolist(), output]).T\n",
    "        togetherFinal.append(togetherOutput)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Use Weekly_Return to calculate sigmaE\n",
    "    sigma_E_weekly = []\n",
    "    for ye in range(len(year)):\n",
    "        sigma_E_weekly.append(sigmaE(nList[ye], square_sum_weekly[ye], sum_weekly[ye]))\n",
    "    eAndSigmaEweekly = pd.DataFrame([lastDate,equity,sigma_E_weekly,rf_last]).T\n",
    "    eAndSigmaEweekly.columns = ['end_date','E','sigmaE','r']\n",
    "    \n",
    "    totalDfWeekly = dpFrame.merge(debt, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(bps, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(weeklyReturn, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(eAndSigmaEweekly, on='end_date',how='left')\n",
    "    finalDfWeekly = totalDfWeekly.dropna(axis=0,how='any')\n",
    "    \n",
    "\n",
    "    outputWeekly = []\n",
    "    longthWeekly = finalDfWeekly.shape[0]\n",
    "    compcodeWeekly = []\n",
    "    for ye in range(longthWeekly):\n",
    "        compcodeWeekly.append(i)\n",
    "        EquityWeekly = finalDfWeekly['E'].tolist()[ye]\n",
    "        SigmaEWeekly = finalDfWeekly['sigmaE'].tolist()[ye]\n",
    "        DWeekly = finalDfWeekly['D'].tolist()[ye]\n",
    "        rWeekly = finalDfWeekly['r'].tolist()[ye]/100\n",
    "        mediumWeekly = f(EquityWeekly,SigmaEWeekly,DWeekly,rWeekly,1)\n",
    "        if mediumWeekly[0] == 0 or mediumWeekly[1] == 0:\n",
    "            outputWeekly.append('-')\n",
    "        else:\n",
    "            dd = (mediumWeekly[0] - finalDfWeekly['DP50'].tolist()[ye])/(mediumWeekly[0]*mediumWeekly[1])\n",
    "            outputWeekly.append(1-N(dd))\n",
    "    outputDfWeekly = pd.DataFrame([finalDfWeekly['end_date'].tolist(),outputWeekly]).T\n",
    "    outputDfWeekly.columns = ['end_date','default_probability']\n",
    "    outputDfWeekly.to_excel(i+'_weekly.xlsx', index=False, encoding='utf_8_sig')\n",
    "    \n",
    "    if len(compcodeWeekly) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        togetherWeeklyOutput = pd.DataFrame([compcodeWeekly, finalDfWeekly['end_date'].tolist(), outputWeekly]).T\n",
    "        togetherWeeklyFinal.append(togetherWeeklyOutput)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39462f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputTogether = togetherFinal\n",
    "outputWeeklyTogether = togetherWeeklyFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05982793",
   "metadata": {},
   "outputs": [],
   "source": [
    "togetherDf = pd.concat(outputTogether)\n",
    "togetherDf = togetherDf.drop_duplicates()\n",
    "togetherDf.columns = ['compcode','end_date','default_probability']\n",
    "togetherDf.to_excel('together_final.xlsx', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ef182",
   "metadata": {},
   "outputs": [],
   "source": [
    "togetherWeeklyDf = pd.concat(outputWeeklyTogether)\n",
    "togetherWeeklyDf = togetherWeeklyDf.drop_duplicates()\n",
    "togetherWeeklyDf.columns = ['compcode','end_date','default_probability']\n",
    "togetherWeeklyDf.to_excel('together_weekly_return_final.xlsx', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29725a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc9985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combasData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Default_Data/output_ln_return_CSMAR/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d3b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data from CSMAR\n",
    "togetherFinalCSMAR = []\n",
    "togetherWeeklyFinalCSMAR = []\n",
    "for i in stockCode_SHSZ:\n",
    "    print(i)\n",
    "    # Debt\n",
    "    debtUse = combasData[combasData['Stkcd'] == i[:6]]\n",
    "    shortTerm = debtUse['current_debt'].tolist()\n",
    "    longTerm = debtUse['long_term_debt'].tolist()\n",
    "    totalDebt = debtUse['total_debt'].tolist()\n",
    "    endDate = [x.replace('-','') for x in debtUse['end_date'].tolist()]\n",
    "\n",
    "    \n",
    "    # Calculate three kinds of DP\n",
    "    dp = [] # DP\n",
    "    dp_50 = [] # DP50\n",
    "    dp_75 = [] # DP75\n",
    "    d_facevalue = [] # D\n",
    "    for y in year:\n",
    "        for e in endDate:\n",
    "            if e.startswith(y) == True:\n",
    "                current = shortTerm[endDate.index(e)]\n",
    "                long = longTerm[endDate.index(e)]\n",
    "                if (current == None) or (long == None):\n",
    "                    dp.append(nan)\n",
    "                    dp_50.append(nan)\n",
    "                    dp_75.append(nan)\n",
    "                else:\n",
    "                    dp.append(current)\n",
    "                    dp_50.append(current+0.5*long)\n",
    "                    dp_75.append(current+0.75*long)\n",
    "                d_facevalue.append(totalDebt[endDate.index(e)])\n",
    "            else:\n",
    "                pass\n",
    "    dpFrame = pd.DataFrame([lastDate,dp,dp_50,dp_75,d_facevalue]).T\n",
    "    dpFrame.columns = ['end_date','DP','DP50','DP75','D']\n",
    "    \n",
    "    \n",
    "    # BPS\n",
    "    bpsData = pro.fina_indicator(ts_code=i, start_date='20110101', end_date='20210630', fields='ts_code,end_date,bps')\n",
    "    d = dict(zip(bpsData['end_date'],bpsData['bps']))\n",
    "    bpsData = []\n",
    "    for j in d.keys():\n",
    "        if j.endswith('1231') == True:\n",
    "            bpsData.append([j,d[j]])\n",
    "    bpsData = sorted(bpsData, key=lambda x:x[0], reverse=False)\n",
    "    bps = pd.DataFrame(bpsData)\n",
    "    bps.columns = ['end_date','bps']\n",
    "    bpsData = bps['bps'].tolist()\n",
    "\n",
    "    # Weekly Stock Return\n",
    "    closeData = ts.pro_bar(ts_code=i, freq='W', adj='qfq', start_date='20110101', end_date='20210101')\n",
    "    #closeData = pro.weekly(ts_code=i, start_date='20110101', end_date='20210101', fields='trade_date,close,pre_close')\n",
    "    if type(closeData) == pd.DataFrame:\n",
    "        closeData['ln_return'] = [math.log(closeData['close'][i]/closeData['pre_close'][i]) for i in range(len(closeData))]\n",
    "        closeData['weekly_return'] = [(closeData['close'][i] - closeData['pre_close'][i])/closeData['pre_close'][i] for i in range(len(closeData))]\n",
    "        closeData['square_ln'] = [pow(i,2) for i in closeData['ln_return'].tolist()]\n",
    "        closeData['square_weekly'] = [pow(i,2) for i in closeData['weekly_return'].tolist()]\n",
    "        closePrice = closeData['close'].tolist()\n",
    "        lnList = closeData['ln_return'].tolist()\n",
    "        weeklyList = closeData['weekly_return'].tolist()    \n",
    "        squareLnList = closeData['square_ln'].tolist()\n",
    "        squareWeeklyList = closeData['square_weekly'].tolist()\n",
    "        tradeDate = closeData['trade_date'].tolist()\n",
    "    else:\n",
    "        pass\n",
    "  \n",
    "    \n",
    "    # Start to calculate KMV\n",
    "    sum_ln = []\n",
    "    sum_weekly = []\n",
    "    square_sum_ln = []\n",
    "    square_sum_weekly = []\n",
    "    mean_close = []\n",
    "    nList = []\n",
    "    for j in year:\n",
    "        #print(j)\n",
    "        sumln = []\n",
    "        sumweekly = []\n",
    "        squaresumln = []\n",
    "        squaresumweekly = []\n",
    "        meanClosePrice = []\n",
    "        for k in tradeDate:\n",
    "            if k.startswith(j) == True:\n",
    "                sumln.append(lnList[tradeDate.index(k)])\n",
    "                sumweekly.append(weeklyList[tradeDate.index(k)])\n",
    "                squaresumln.append(squareLnList[tradeDate.index(k)])\n",
    "                squaresumweekly.append(squareWeeklyList[tradeDate.index(k)])\n",
    "                meanClosePrice.append(closePrice[tradeDate.index(k)])\n",
    "            else:\n",
    "                pass\n",
    "        sum_ln.append(np.sum(sumln))\n",
    "        sum_weekly.append(np.sum(sumweekly))        \n",
    "        square_sum_ln.append(np.sum(squaresumln))\n",
    "        square_sum_weekly.append(np.sum(squaresumweekly))\n",
    "        if len(meanClosePrice) == 0:\n",
    "            mean_close.append(0)\n",
    "        else:\n",
    "            mean_close.append(np.mean(meanClosePrice))\n",
    "        nList.append(len(sumln))\n",
    "    print('Done')\n",
    "    weeklyReturn = pd.DataFrame([lastDate, sum_ln, sum_weekly, square_sum_ln, square_sum_weekly, mean_close, nList]).T\n",
    "    weeklyReturn.columns = ['end_date','ln_return','weekly_return','square_ln_return','square_weekly_return','mean_close','numOfWeeks']\n",
    "    \n",
    "    # Calculate E\n",
    "    # First, get the number of tradable shares and non-tradable shares\n",
    "    shareData = capchgData.loc[capchgData['Stkcd'] == i]\n",
    "    tradeShare = []\n",
    "    nonTradeShare = []\n",
    "    for l in lastDate:\n",
    "        if l <= shareData['Shrchgdt'].tolist()[0]:\n",
    "            total = 0\n",
    "            trade = 0\n",
    "        else:\n",
    "            total = shareData.loc[shareData['Shrchgdt'] <= l]['Nshrttl'].tolist()[-1]\n",
    "            trade = shareData.loc[shareData['Shrchgdt'] <= l]['Nshra'].tolist()[-1]\n",
    "        nonTrade = total - trade\n",
    "        tradeShare.append(trade)\n",
    "        nonTradeShare.append(nonTrade)\n",
    "    # Then calculate E\n",
    "    equity = [] # E\n",
    "    for r in range(len(bps)):\n",
    "        tradeEquity = mean_close[r] * tradeShare[r]\n",
    "        nonTradeEquity = bpsData[r] * nonTradeShare[r]\n",
    "        equityMktVal = tradeEquity + nonTradeEquity\n",
    "        equity.append(equityMktVal)\n",
    "        \n",
    "    # Use ln_Return to calculate sigmaE\n",
    "    sigma_E_ln = []\n",
    "    for ye in range(len(year)):\n",
    "        sigma_E_ln.append(sigmaE(nList[ye], square_sum_ln[ye], sum_ln[ye]))\n",
    "    eAndSigmaELn = pd.DataFrame([lastDate,equity,sigma_E_ln,rf_last]).T\n",
    "    eAndSigmaELn.columns = ['end_date','E','sigmaE','r']\n",
    "    \n",
    "    totalDf = dpFrame.merge(debt, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(bps, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(weeklyReturn, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(eAndSigmaELn, on='end_date',how='left')\n",
    "    finalDf = totalDf.dropna(axis=0,how='any')\n",
    "    \n",
    "\n",
    "    output = []\n",
    "    longth = finalDf.shape[0]\n",
    "    compcode = []\n",
    "    for ye in range(longth):\n",
    "        compcode.append(i)\n",
    "        Equity = finalDf['E'].tolist()[ye]\n",
    "        SigmaE = finalDf['sigmaE'].tolist()[ye]\n",
    "        D = finalDf['D'].tolist()[ye]\n",
    "        r = finalDf['r'].tolist()[ye]/100\n",
    "        medium =f(Equity,SigmaE,D,r,1)\n",
    "        if medium[0] == 0 or medium[1] == 0:\n",
    "            output.append('-')\n",
    "        else:\n",
    "            dd = (medium[0] - finalDf['DP50'].tolist()[ye])/(medium[0]*medium[1])\n",
    "            output.append(1-N(dd))\n",
    "    outputDf = pd.DataFrame([finalDf['end_date'].tolist(),output]).T\n",
    "    outputDf.columns = ['end_date','default_probability']\n",
    "    outputDf.to_excel(i+'.xlsx', index=False, encoding='utf_8_sig')\n",
    "    \n",
    "    if len(compcode) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        togetherOutput = pd.DataFrame([compcode, finalDf['end_date'].tolist(), output]).T\n",
    "        togetherFinalCSMAR.append(togetherOutput)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Use Weekly_Return to calculate sigmaE\n",
    "    sigma_E_weekly = []\n",
    "    for ye in range(len(year)):\n",
    "        sigma_E_weekly.append(sigmaE(nList[ye], square_sum_weekly[ye], sum_weekly[ye]))\n",
    "    eAndSigmaEweekly = pd.DataFrame([lastDate,equity,sigma_E_weekly,rf_last]).T\n",
    "    eAndSigmaEweekly.columns = ['end_date','E','sigmaE','r']\n",
    "    \n",
    "    totalDfWeekly = dpFrame.merge(debt, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(bps, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(weeklyReturn, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(eAndSigmaEweekly, on='end_date',how='left')\n",
    "    finalDfWeekly = totalDfWeekly.dropna(axis=0,how='any')\n",
    "    \n",
    "\n",
    "    outputWeekly = []\n",
    "    longthWeekly = finalDfWeekly.shape[0]\n",
    "    compcodeWeekly = []\n",
    "    for ye in range(longthWeekly):\n",
    "        compcodeWeekly.append(i)\n",
    "        EquityWeekly = finalDfWeekly['E'].tolist()[ye]\n",
    "        SigmaEWeekly = finalDfWeekly['sigmaE'].tolist()[ye]\n",
    "        DWeekly = finalDfWeekly['D'].tolist()[ye]\n",
    "        rWeekly = finalDfWeekly['r'].tolist()[ye]/100\n",
    "        mediumWeekly = f(EquityWeekly,SigmaEWeekly,DWeekly,rWeekly,1)\n",
    "        if mediumWeekly[0] == 0 or mediumWeekly[1] == 0:\n",
    "            outputWeekly.append('-')\n",
    "        else:\n",
    "            dd = (mediumWeekly[0] - finalDfWeekly['DP50'].tolist()[ye])/(mediumWeekly[0]*mediumWeekly[1])\n",
    "            outputWeekly.append(1-N(dd))\n",
    "    outputDfWeekly = pd.DataFrame([finalDfWeekly['end_date'].tolist(),outputWeekly]).T\n",
    "    outputDfWeekly.columns = ['end_date','default_probability']\n",
    "    outputDfWeekly.to_excel(i+'_weekly.xlsx', index=False, encoding='utf_8_sig')\n",
    "    \n",
    "    if len(compcodeWeekly) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        togetherWeeklyOutput = pd.DataFrame([compcodeWeekly, finalDfWeekly['end_date'].tolist(), outputWeekly]).T\n",
    "        togetherWeeklyFinalCSMAR.append(togetherWeeklyOutput)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff412ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "outputTogetherCSMAR = togetherFinalCSMAR\n",
    "outputWeeklyTogetherCSMAR = togetherWeeklyFinalCSMAR\n",
    "togetherDfCSMAR = pd.concat(outputTogetherCSMAR)\n",
    "togetherDfCSMAR = togetherDfCSMAR.drop_duplicates()\n",
    "togetherDfCSMAR.columns = ['compcode','end_date','default_probability']\n",
    "togetherDfCSMAR.to_excel('together_final_CSMAR.xlsx', index=False, encoding='utf_8_sig')\n",
    "togetherWeeklyDfCSMAR = pd.concat(outputWeeklyTogetherCSMAR)\n",
    "togetherWeeklyDfCSMAR = togetherWeeklyDfCSMAR.drop_duplicates()\n",
    "togetherWeeklyDfCSMAR.columns = ['compcode','end_date','default_probability']\n",
    "togetherWeeklyDfCSMAR.to_excel('together_weekly_return_final_CSMAR.xlsx', index=False, encoding='utf_8_sig')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3811e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary explanation\n",
    "'''\n",
    "Tushare data results:\n",
    "togetherDf for ln return\n",
    "togetherWeeklyDf for weekly return\n",
    "\n",
    "CSMAR data results:\n",
    "togetherDfCSMAR for ln return\n",
    "togetherWeeklyDfCSMAR for weekly return\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa068c7",
   "metadata": {},
   "source": [
    "# Load in Default Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607cc5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultData = pd.read_excel('default_SHSZ.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultCompcode = defaultData['tsCode'].tolist()\n",
    "defaultCompdate = defaultData['compdate'].tolist()\n",
    "\n",
    "newDef_compdate = []\n",
    "for i in defaultCompdate:\n",
    "    if type(i) == datetime.datetime:\n",
    "        newDef_compdate.append(time.strftime(\"%Y%m%d\",time.strptime(str(i), \"%Y-%m-%d %H:%M:%S\")))\n",
    "    else:\n",
    "        newDef_compdate.append(time.strftime(\"%Y%m%d\",time.strptime(i, \"%m/%d/%Y\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tushare data result merge\n",
    "framesln = []\n",
    "framesWeekly = []\n",
    "for i in range(len(defaultCompcode)):\n",
    "    df_ln = togetherDf[togetherDf['compcode'] == defaultCompcode[i]]\n",
    "    df_ln['default_date'] = newDef_compdate[i]\n",
    "    framesln.append(df_ln)\n",
    "    df_weekly = togetherWeeklyDf[togetherWeeklyDf['compcode'] == defaultCompcode[i]]\n",
    "    df_weekly['default_date'] = newDef_compdate[i]\n",
    "    framesWeekly.append(df_weekly)\n",
    "defaultExtractln = pd.concat(framesln)\n",
    "defaultExtractln = defaultExtractln.drop_duplicates()\n",
    "defaultExtractWeekly = pd.concat(framesWeekly)\n",
    "defaultExtractWeekly = defaultExtractWeekly.drop_duplicates()\n",
    "os.chdir('Default_Data/')\n",
    "defaultExtractln.to_excel('default_prob_summary_ln.xlsx', index=False, encoding='utf_8_sig')\n",
    "defaultExtractWeekly.to_excel('default_prob_summary_weekly.xlsx', index=False, encoding='utf_8_sig')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3abf26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default Result with ln return, rf_last and Tushare debt data\n",
    "defaultLnLastTuFrame = []\n",
    "for i in list(set(defaultExtractln['compcode'].tolist())):\n",
    "    result = defaultExtractln[defaultExtractln['compcode'] == i]\n",
    "    if (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231') in result['end_date'].tolist():\n",
    "        defaultLnLastTuFrame.append(result[result['end_date'] == (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231')])\n",
    "    else:\n",
    "        pass\n",
    "defaultLnLastTu = pd.concat(defaultLnLastTuFrame)\n",
    "os.chdir('Default_Data/final_result/')\n",
    "defaultLnLastTu.to_excel('defaultLnLastTushare.xlsx', index=False, encoding='sig_utf_8')\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plt.bar(defaultLnLastTu['compcode'].tolist(), defaultLnLastTu['default_probability'].tolist())\n",
    "fig.savefig('defaultLnLastTushare.jpg', dpi = 750)\n",
    "plt.close()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1565b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Result with weekly return, rf_last and Tushare debt data\n",
    "defaultWeeklyLastTuFrame = []\n",
    "for i in list(set(defaultExtractWeekly['compcode'].tolist())):\n",
    "    result = defaultExtractWeekly[defaultExtractWeekly['compcode'] == i]\n",
    "    if (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231') in result['end_date'].tolist():\n",
    "        defaultWeeklyLastTuFrame.append(result[result['end_date'] == (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231')])\n",
    "    else:\n",
    "        pass\n",
    "defaultWeeklyLastTu = pd.concat(defaultWeeklyLastTuFrame)\n",
    "os.chdir('Default_Data/final_result/')\n",
    "defaultWeeklyLastTu.to_excel('defaultWeeklyLastTushare.xlsx', index=False, encoding='sig_utf_8')\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plt.bar(defaultWeeklyLastTu['compcode'].tolist(), defaultWeeklyLastTu['default_probability'].tolist())\n",
    "fig.savefig('defaultWeeklyLastTushare.jpg', dpi = 750)\n",
    "plt.close()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3dcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSMAR data result merge\n",
    "frameslnCSMAR = []\n",
    "framesWeeklyCSMAR = []\n",
    "for i in range(len(defaultCompcode)):\n",
    "    df_ln_CSMAR = togetherDfCSMAR[togetherDfCSMAR['compcode'] == defaultCompcode[i]]\n",
    "    df_ln_CSMAR['default_date'] = newDef_compdate[i]\n",
    "    frameslnCSMAR.append(df_ln_CSMAR)\n",
    "    df_weekly_CSMAR = togetherWeeklyDfCSMAR[togetherWeeklyDfCSMAR['compcode'] == defaultCompcode[i]]\n",
    "    df_weekly_CSMAR['default_date'] = newDef_compdate[i]\n",
    "    framesWeeklyCSMAR.append(df_weekly_CSMAR)\n",
    "defaultExtractlnCSMAR = pd.concat(frameslnCSMAR)\n",
    "defaultExtractlnCSMAR = defaultExtractlnCSMAR.drop_duplicates()\n",
    "defaultExtractWeeklyCSMAR = pd.concat(framesWeeklyCSMAR)\n",
    "defaultExtractWeeklyCSMAR = defaultExtractWeeklyCSMAR.drop_duplicates()\n",
    "os.chdir('Default_Data/')\n",
    "defaultExtractlnCSMAR.to_excel('default_prob_summary_ln_CSMAR.xlsx', index=False, encoding='utf_8_sig')\n",
    "defaultExtractWeeklyCSMAR.to_excel('default_prob_summary_weekly_CSMAR.xlsx', index=False, encoding='utf_8_sig')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b789894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Result with ln return, rf_last and CSMAR debt data\n",
    "defaultLnLastCSMARFrame = []\n",
    "for i in list(set(defaultExtractlnCSMAR['compcode'].tolist())):\n",
    "    result = defaultExtractlnCSMAR[defaultExtractlnCSMAR['compcode'] == i]\n",
    "    if (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231') in result['end_date'].tolist():\n",
    "        defaultLnLastCSMARFrame.append(result[result['end_date'] == (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231')])\n",
    "    else:\n",
    "        pass\n",
    "defaultLnLastCSMAR = pd.concat(defaultLnLastCSMARFrame)\n",
    "os.chdir('Default_Data/final_result/')\n",
    "defaultLnLastCSMAR.to_excel('defaultLnLastCSMAR.xlsx', index=False, encoding='sig_utf_8')\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plt.bar(defaultLnLastCSMAR['compcode'].tolist(), defaultLnLastCSMAR['default_probability'].tolist())\n",
    "fig.savefig('defaultLnLastCSMAR.jpg', dpi = 750)\n",
    "plt.close()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2bb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Result with weekly return, rf_last and CSMAR debt data\n",
    "defaultWeeklyLastCSMARFrame = []\n",
    "for i in list(set(defaultExtractWeeklyCSMAR['compcode'].tolist())):\n",
    "    result = defaultExtractWeeklyCSMAR[defaultExtractWeeklyCSMAR['compcode'] == i]\n",
    "    if (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231') in result['end_date'].tolist():\n",
    "        defaultWeeklyLastCSMARFrame.append(result[result['end_date'] == (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231')])\n",
    "    else:\n",
    "        pass\n",
    "defaultWeeklyLastCSMAR = pd.concat(defaultWeeklyLastCSMARFrame)\n",
    "os.chdir('Default_Data/final_result/')\n",
    "defaultWeeklyLastCSMAR.to_excel('defaultWeeklyLastCSMAR.xlsx', index=False, encoding='sig_utf_8')\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plt.bar(defaultWeeklyLastCSMAR['compcode'].tolist(), defaultWeeklyLastCSMAR['default_probability'].tolist())\n",
    "fig.savefig('defaultWeeklyLastCSMAR.jpg', dpi = 750)\n",
    "plt.close()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334dc83",
   "metadata": {},
   "source": [
    "# Use rf_mean as rf, and rerunning the whole things (Using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5bd2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Default_Data/output_rf_mean/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f895ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data from Tushare\n",
    "togetherFinal_rfmean = []\n",
    "togetherWeeklyFinal_rfmean = []\n",
    "for i in stockCode_SHSZ:\n",
    "    print(i)\n",
    "    # Debt\n",
    "    debtData = pro.balancesheet(ts_code=i, start_date='20110101', end_date='20210630', fields='end_date,total_cur_liab,total_ncl,total_liab')\n",
    "    debtNew=debtData[debtData['end_date'].apply(lambda x:x[4:])=='1231']\n",
    "    debt=debtNew.drop_duplicates()\n",
    "    shortTerm = debt['total_cur_liab'].tolist()\n",
    "    longTerm = debt['total_ncl'].tolist()\n",
    "    totalDebt = debt['total_liab'].tolist()\n",
    "    endDate = debt['end_date'].tolist()\n",
    "    \n",
    "    # Calculate three kinds of DP\n",
    "    dp = [] # DP\n",
    "    dp_50 = [] # DP50\n",
    "    dp_75 = [] # DP75\n",
    "    d_facevalue = [] # D\n",
    "    for y in year:\n",
    "        for e in endDate:\n",
    "            if e.startswith(y) == True:\n",
    "                current = shortTerm[endDate.index(e)]\n",
    "                long = longTerm[endDate.index(e)]\n",
    "                if (current == None) or (long == None):\n",
    "                    dp.append(nan)\n",
    "                    dp_50.append(nan)\n",
    "                    dp_75.append(nan)\n",
    "                else:\n",
    "                    dp.append(current)\n",
    "                    dp_50.append(current+0.5*long)\n",
    "                    dp_75.append(current+0.75*long)\n",
    "                d_facevalue.append(totalDebt[endDate.index(e)])\n",
    "            else:\n",
    "                pass\n",
    "    dpFrame = pd.DataFrame([lastDate,dp,dp_50,dp_75,d_facevalue]).T\n",
    "    dpFrame.columns = ['end_date','DP','DP50','DP75','D']\n",
    "    \n",
    "    \n",
    "    # BPS\n",
    "    bpsData = pro.fina_indicator(ts_code=i, start_date='20110101', end_date='20210630', fields='ts_code,end_date,bps')\n",
    "    d = dict(zip(bpsData['end_date'],bpsData['bps']))\n",
    "    bpsData = []\n",
    "    for j in d.keys():\n",
    "        if j.endswith('1231') == True:\n",
    "            bpsData.append([j,d[j]])\n",
    "    bpsData = sorted(bpsData, key=lambda x:x[0], reverse=False)\n",
    "    bps = pd.DataFrame(bpsData)\n",
    "    bps.columns = ['end_date','bps']\n",
    "    bpsData = bps['bps'].tolist()\n",
    "\n",
    "    # Weekly Stock Return\n",
    "    closeData = ts.pro_bar(ts_code=i, freq='W', adj='qfq', start_date='20110101', end_date='20210101')\n",
    "    #closeData = pro.weekly(ts_code=i, start_date='20110101', end_date='20210101', fields='trade_date,close,pre_close')\n",
    "    if type(closeData) == pd.DataFrame:\n",
    "        closeData['ln_return'] = [math.log(closeData['close'][i]/closeData['pre_close'][i]) for i in range(len(closeData))]\n",
    "        closeData['weekly_return'] = [(closeData['close'][i] - closeData['pre_close'][i])/closeData['pre_close'][i] for i in range(len(closeData))]\n",
    "        closeData['square_ln'] = [pow(i,2) for i in closeData['ln_return'].tolist()]\n",
    "        closeData['square_weekly'] = [pow(i,2) for i in closeData['weekly_return'].tolist()]\n",
    "        closePrice = closeData['close'].tolist()\n",
    "        lnList = closeData['ln_return'].tolist()\n",
    "        weeklyList = closeData['weekly_return'].tolist()    \n",
    "        squareLnList = closeData['square_ln'].tolist()\n",
    "        squareWeeklyList = closeData['square_weekly'].tolist()\n",
    "        tradeDate = closeData['trade_date'].tolist()\n",
    "    else:\n",
    "        pass\n",
    "  \n",
    "    \n",
    "    # Start to calculate KMV\n",
    "    sum_ln = []\n",
    "    sum_weekly = []\n",
    "    square_sum_ln = []\n",
    "    square_sum_weekly = []\n",
    "    mean_close = []\n",
    "    nList = []\n",
    "    for j in year:\n",
    "        #print(j)\n",
    "        sumln = []\n",
    "        sumweekly = []\n",
    "        squaresumln = []\n",
    "        squaresumweekly = []\n",
    "        meanClosePrice = []\n",
    "        for k in tradeDate:\n",
    "            if k.startswith(j) == True:\n",
    "                sumln.append(lnList[tradeDate.index(k)])\n",
    "                sumweekly.append(weeklyList[tradeDate.index(k)])\n",
    "                squaresumln.append(squareLnList[tradeDate.index(k)])\n",
    "                squaresumweekly.append(squareWeeklyList[tradeDate.index(k)])\n",
    "                meanClosePrice.append(closePrice[tradeDate.index(k)])\n",
    "            else:\n",
    "                pass\n",
    "        sum_ln.append(np.sum(sumln))\n",
    "        sum_weekly.append(np.sum(sumweekly))        \n",
    "        square_sum_ln.append(np.sum(squaresumln))\n",
    "        square_sum_weekly.append(np.sum(squaresumweekly))\n",
    "        if len(meanClosePrice) == 0:\n",
    "            mean_close.append(0)\n",
    "        else:\n",
    "            mean_close.append(np.mean(meanClosePrice))\n",
    "        nList.append(len(sumln))\n",
    "    print('Done')\n",
    "    weeklyReturn = pd.DataFrame([lastDate, sum_ln, sum_weekly, square_sum_ln, square_sum_weekly, mean_close, nList]).T\n",
    "    weeklyReturn.columns = ['end_date','ln_return','weekly_return','square_ln_return','square_weekly_return','mean_close','numOfWeeks']\n",
    "    \n",
    "    # Calculate E\n",
    "    # First, get the number of tradable shares and non-tradable shares\n",
    "    shareData = capchgData.loc[capchgData['Stkcd'] == i]\n",
    "    tradeShare = []\n",
    "    nonTradeShare = []\n",
    "    for l in lastDate:\n",
    "        if l <= shareData['Shrchgdt'].tolist()[0]:\n",
    "            total = 0\n",
    "            trade = 0\n",
    "        else:\n",
    "            total = shareData.loc[shareData['Shrchgdt'] <= l]['Nshrttl'].tolist()[-1]\n",
    "            trade = shareData.loc[shareData['Shrchgdt'] <= l]['Nshra'].tolist()[-1]\n",
    "        nonTrade = total - trade\n",
    "        tradeShare.append(trade)\n",
    "        nonTradeShare.append(nonTrade)\n",
    "    # Then calculate E\n",
    "    equity = [] # E\n",
    "    for r in range(len(bps)):\n",
    "        tradeEquity = mean_close[r] * tradeShare[r]\n",
    "        nonTradeEquity = bpsData[r] * nonTradeShare[r]\n",
    "        equityMktVal = tradeEquity + nonTradeEquity\n",
    "        equity.append(equityMktVal)\n",
    "        \n",
    "    # Use ln_Return to calculate sigmaE\n",
    "    sigma_E_ln = []\n",
    "    for ye in range(len(year)):\n",
    "        sigma_E_ln.append(sigmaE(nList[ye], square_sum_ln[ye], sum_ln[ye]))\n",
    "    eAndSigmaELn = pd.DataFrame([lastDate,equity,sigma_E_ln,rf_mean]).T\n",
    "    eAndSigmaELn.columns = ['end_date','E','sigmaE','r']\n",
    "    \n",
    "    totalDf = dpFrame.merge(debt, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(bps, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(weeklyReturn, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(eAndSigmaELn, on='end_date',how='left')\n",
    "    finalDf = totalDf.dropna(axis=0,how='any')\n",
    "    \n",
    "\n",
    "    output = []\n",
    "    longth = finalDf.shape[0]\n",
    "    compcode = []\n",
    "    for ye in range(longth):\n",
    "        compcode.append(i)\n",
    "        Equity = finalDf['E'].tolist()[ye]\n",
    "        SigmaE = finalDf['sigmaE'].tolist()[ye]\n",
    "        D = finalDf['D'].tolist()[ye]\n",
    "        r = finalDf['r'].tolist()[ye]/100\n",
    "        medium =f(Equity,SigmaE,D,r,1)\n",
    "        if medium[0] == 0 or medium[1] == 0:\n",
    "            output.append('-')\n",
    "        else:\n",
    "            dd = (medium[0] - finalDf['DP50'].tolist()[ye])/(medium[0]*medium[1])\n",
    "            output.append(1-N(dd))\n",
    "    outputDf = pd.DataFrame([finalDf['end_date'].tolist(),output]).T\n",
    "    outputDf.columns = ['end_date','default_probability']\n",
    "    outputDf.to_excel(i+'.xlsx', index=False, encoding='utf_8_sig')\n",
    "    \n",
    "    if len(compcode) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        togetherOutput = pd.DataFrame([compcode, finalDf['end_date'].tolist(), output]).T\n",
    "        togetherFinal_rfmean.append(togetherOutput)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Use Weekly_Return to calculate sigmaE\n",
    "    sigma_E_weekly = []\n",
    "    for ye in range(len(year)):\n",
    "        sigma_E_weekly.append(sigmaE(nList[ye], square_sum_weekly[ye], sum_weekly[ye]))\n",
    "    eAndSigmaEweekly = pd.DataFrame([lastDate,equity,sigma_E_weekly,rf_mean]).T\n",
    "    eAndSigmaEweekly.columns = ['end_date','E','sigmaE','r']\n",
    "    \n",
    "    totalDfWeekly = dpFrame.merge(debt, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(bps, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(weeklyReturn, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(eAndSigmaEweekly, on='end_date',how='left')\n",
    "    finalDfWeekly = totalDfWeekly.dropna(axis=0,how='any')\n",
    "    \n",
    "\n",
    "    outputWeekly = []\n",
    "    longthWeekly = finalDfWeekly.shape[0]\n",
    "    compcodeWeekly = []\n",
    "    for ye in range(longthWeekly):\n",
    "        compcodeWeekly.append(i)\n",
    "        EquityWeekly = finalDfWeekly['E'].tolist()[ye]\n",
    "        SigmaEWeekly = finalDfWeekly['sigmaE'].tolist()[ye]\n",
    "        DWeekly = finalDfWeekly['D'].tolist()[ye]\n",
    "        rWeekly = finalDfWeekly['r'].tolist()[ye]/100\n",
    "        mediumWeekly = f(EquityWeekly,SigmaEWeekly,DWeekly,rWeekly,1)\n",
    "        if mediumWeekly[0] == 0 or mediumWeekly[1] == 0:\n",
    "            outputWeekly.append('-')\n",
    "        else:\n",
    "            dd = (mediumWeekly[0] - finalDfWeekly['DP50'].tolist()[ye])/(mediumWeekly[0]*mediumWeekly[1])\n",
    "            outputWeekly.append(1-N(dd))\n",
    "    outputDfWeekly = pd.DataFrame([finalDfWeekly['end_date'].tolist(),outputWeekly]).T\n",
    "    outputDfWeekly.columns = ['end_date','default_probability']\n",
    "    outputDfWeekly.to_excel(i+'_weekly.xlsx', index=False, encoding='utf_8_sig')\n",
    "    \n",
    "    if len(compcodeWeekly) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        togetherWeeklyOutput = pd.DataFrame([compcodeWeekly, finalDfWeekly['end_date'].tolist(), outputWeekly]).T\n",
    "        togetherWeeklyFinal_rfmean.append(togetherWeeklyOutput)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ab585",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "outputTogether_rfmean = togetherFinal_rfmean\n",
    "outputWeeklyTogether_rfmean = togetherWeeklyFinal_rfmean\n",
    "togetherDf_rfmean = pd.concat(outputTogether_rfmean)\n",
    "togetherDf_rfmean = togetherDf_rfmean.drop_duplicates()\n",
    "togetherDf_rfmean.columns = ['compcode','end_date','default_probability']\n",
    "togetherDf_rfmean.to_excel('together_final_rfmean.xlsx', index=False, encoding='utf_8_sig')\n",
    "togetherWeeklyDf_rfmean = pd.concat(outputWeeklyTogether_rfmean)\n",
    "togetherWeeklyDf_rfmean = togetherWeeklyDf_rfmean.drop_duplicates()\n",
    "togetherWeeklyDf_rfmean.columns = ['compcode','end_date','default_probability']\n",
    "togetherWeeklyDf_rfmean.to_excel('together_weekly_return_final_rfmean.xlsx', index=False, encoding='utf_8_sig')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfaa148",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Default_Data/output_rf_mean_CSMAR/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeef235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data from CSMAR\n",
    "togetherFinalCSMAR_rfmean = []\n",
    "togetherWeeklyFinalCSMAR_rfmean = []\n",
    "for i in stockCode_SHSZ:\n",
    "    print(i)\n",
    "    # Debt\n",
    "    debtUse = combasData[combasData['Stkcd'] == i[:6]]\n",
    "    shortTerm = debtUse['current_debt'].tolist()\n",
    "    longTerm = debtUse['long_term_debt'].tolist()\n",
    "    totalDebt = debtUse['total_debt'].tolist()\n",
    "    endDate = [x.replace('-','') for x in debtUse['end_date'].tolist()]\n",
    "\n",
    "    \n",
    "    # Calculate three kinds of DP\n",
    "    dp = [] # DP\n",
    "    dp_50 = [] # DP50\n",
    "    dp_75 = [] # DP75\n",
    "    d_facevalue = [] # D\n",
    "    for y in year:\n",
    "        for e in endDate:\n",
    "            if e.startswith(y) == True:\n",
    "                current = shortTerm[endDate.index(e)]\n",
    "                long = longTerm[endDate.index(e)]\n",
    "                if (current == None) or (long == None):\n",
    "                    dp.append(nan)\n",
    "                    dp_50.append(nan)\n",
    "                    dp_75.append(nan)\n",
    "                else:\n",
    "                    dp.append(current)\n",
    "                    dp_50.append(current+0.5*long)\n",
    "                    dp_75.append(current+0.75*long)\n",
    "                d_facevalue.append(totalDebt[endDate.index(e)])\n",
    "            else:\n",
    "                pass\n",
    "    dpFrame = pd.DataFrame([lastDate,dp,dp_50,dp_75,d_facevalue]).T\n",
    "    dpFrame.columns = ['end_date','DP','DP50','DP75','D']\n",
    "    \n",
    "    \n",
    "    # BPS\n",
    "    bpsData = pro.fina_indicator(ts_code=i, start_date='20110101', end_date='20210630', fields='ts_code,end_date,bps')\n",
    "    d = dict(zip(bpsData['end_date'],bpsData['bps']))\n",
    "    bpsData = []\n",
    "    for j in d.keys():\n",
    "        if j.endswith('1231') == True:\n",
    "            bpsData.append([j,d[j]])\n",
    "    bpsData = sorted(bpsData, key=lambda x:x[0], reverse=False)\n",
    "    bps = pd.DataFrame(bpsData)\n",
    "    bps.columns = ['end_date','bps']\n",
    "    bpsData = bps['bps'].tolist()\n",
    "\n",
    "    # Weekly Stock Return\n",
    "    closeData = ts.pro_bar(ts_code=i, freq='W', adj='qfq', start_date='20110101', end_date='20210101')\n",
    "    #closeData = pro.weekly(ts_code=i, start_date='20110101', end_date='20210101', fields='trade_date,close,pre_close')\n",
    "    if type(closeData) == pd.DataFrame:\n",
    "        closeData['ln_return'] = [math.log(closeData['close'][i]/closeData['pre_close'][i]) for i in range(len(closeData))]\n",
    "        closeData['weekly_return'] = [(closeData['close'][i] - closeData['pre_close'][i])/closeData['pre_close'][i] for i in range(len(closeData))]\n",
    "        closeData['square_ln'] = [pow(i,2) for i in closeData['ln_return'].tolist()]\n",
    "        closeData['square_weekly'] = [pow(i,2) for i in closeData['weekly_return'].tolist()]\n",
    "        closePrice = closeData['close'].tolist()\n",
    "        lnList = closeData['ln_return'].tolist()\n",
    "        weeklyList = closeData['weekly_return'].tolist()    \n",
    "        squareLnList = closeData['square_ln'].tolist()\n",
    "        squareWeeklyList = closeData['square_weekly'].tolist()\n",
    "        tradeDate = closeData['trade_date'].tolist()\n",
    "    else:\n",
    "        pass\n",
    "  \n",
    "    \n",
    "    # Start to calculate KMV\n",
    "    sum_ln = []\n",
    "    sum_weekly = []\n",
    "    square_sum_ln = []\n",
    "    square_sum_weekly = []\n",
    "    mean_close = []\n",
    "    nList = []\n",
    "    for j in year:\n",
    "        #print(j)\n",
    "        sumln = []\n",
    "        sumweekly = []\n",
    "        squaresumln = []\n",
    "        squaresumweekly = []\n",
    "        meanClosePrice = []\n",
    "        for k in tradeDate:\n",
    "            if k.startswith(j) == True:\n",
    "                sumln.append(lnList[tradeDate.index(k)])\n",
    "                sumweekly.append(weeklyList[tradeDate.index(k)])\n",
    "                squaresumln.append(squareLnList[tradeDate.index(k)])\n",
    "                squaresumweekly.append(squareWeeklyList[tradeDate.index(k)])\n",
    "                meanClosePrice.append(closePrice[tradeDate.index(k)])\n",
    "            else:\n",
    "                pass\n",
    "        sum_ln.append(np.sum(sumln))\n",
    "        sum_weekly.append(np.sum(sumweekly))        \n",
    "        square_sum_ln.append(np.sum(squaresumln))\n",
    "        square_sum_weekly.append(np.sum(squaresumweekly))\n",
    "        if len(meanClosePrice) == 0:\n",
    "            mean_close.append(0)\n",
    "        else:\n",
    "            mean_close.append(np.mean(meanClosePrice))\n",
    "        nList.append(len(sumln))\n",
    "    print('Done')\n",
    "    weeklyReturn = pd.DataFrame([lastDate, sum_ln, sum_weekly, square_sum_ln, square_sum_weekly, mean_close, nList]).T\n",
    "    weeklyReturn.columns = ['end_date','ln_return','weekly_return','square_ln_return','square_weekly_return','mean_close','numOfWeeks']\n",
    "    \n",
    "    # Calculate E\n",
    "    # First, get the number of tradable shares and non-tradable shares\n",
    "    shareData = capchgData.loc[capchgData['Stkcd'] == i]\n",
    "    tradeShare = []\n",
    "    nonTradeShare = []\n",
    "    for l in lastDate:\n",
    "        if l <= shareData['Shrchgdt'].tolist()[0]:\n",
    "            total = 0\n",
    "            trade = 0\n",
    "        else:\n",
    "            total = shareData.loc[shareData['Shrchgdt'] <= l]['Nshrttl'].tolist()[-1]\n",
    "            trade = shareData.loc[shareData['Shrchgdt'] <= l]['Nshra'].tolist()[-1]\n",
    "        nonTrade = total - trade\n",
    "        tradeShare.append(trade)\n",
    "        nonTradeShare.append(nonTrade)\n",
    "    # Then calculate E\n",
    "    equity = [] # E\n",
    "    for r in range(len(bps)):\n",
    "        tradeEquity = mean_close[r] * tradeShare[r]\n",
    "        nonTradeEquity = bpsData[r] * nonTradeShare[r]\n",
    "        equityMktVal = tradeEquity + nonTradeEquity\n",
    "        equity.append(equityMktVal)\n",
    "        \n",
    "    # Use ln_Return to calculate sigmaE\n",
    "    sigma_E_ln = []\n",
    "    for ye in range(len(year)):\n",
    "        sigma_E_ln.append(sigmaE(nList[ye], square_sum_ln[ye], sum_ln[ye]))\n",
    "    eAndSigmaELn = pd.DataFrame([lastDate,equity,sigma_E_ln,rf_mean]).T\n",
    "    eAndSigmaELn.columns = ['end_date','E','sigmaE','r']\n",
    "    \n",
    "    totalDf = dpFrame.merge(debt, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(bps, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(weeklyReturn, on='end_date',how='left')\n",
    "    totalDf = totalDf.merge(eAndSigmaELn, on='end_date',how='left')\n",
    "    finalDf = totalDf.dropna(axis=0,how='any')\n",
    "    \n",
    "\n",
    "    output = []\n",
    "    longth = finalDf.shape[0]\n",
    "    compcode = []\n",
    "    for ye in range(longth):\n",
    "        compcode.append(i)\n",
    "        Equity = finalDf['E'].tolist()[ye]\n",
    "        SigmaE = finalDf['sigmaE'].tolist()[ye]\n",
    "        D = finalDf['D'].tolist()[ye]\n",
    "        r = finalDf['r'].tolist()[ye]/100\n",
    "        medium =f(Equity,SigmaE,D,r,1)\n",
    "        if medium[0] == 0 or medium[1] == 0:\n",
    "            output.append('-')\n",
    "        else:\n",
    "            dd = (medium[0] - finalDf['DP50'].tolist()[ye])/(medium[0]*medium[1])\n",
    "            output.append(1-N(dd))\n",
    "    outputDf = pd.DataFrame([finalDf['end_date'].tolist(),output]).T\n",
    "    outputDf.columns = ['end_date','default_probability']\n",
    "    outputDf.to_excel(i+'.xlsx', index=False, encoding='utf_8_sig')\n",
    "    \n",
    "    if len(compcode) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        togetherOutput = pd.DataFrame([compcode, finalDf['end_date'].tolist(), output]).T\n",
    "        togetherFinalCSMAR_rfmean.append(togetherOutput)\n",
    "\n",
    "    # Use Weekly_Return to calculate sigmaE\n",
    "    sigma_E_weekly = []\n",
    "    for ye in range(len(year)):\n",
    "        sigma_E_weekly.append(sigmaE(nList[ye], square_sum_weekly[ye], sum_weekly[ye]))\n",
    "    eAndSigmaEweekly = pd.DataFrame([lastDate,equity,sigma_E_weekly,rf_mean]).T\n",
    "    eAndSigmaEweekly.columns = ['end_date','E','sigmaE','r']\n",
    "    \n",
    "    totalDfWeekly = dpFrame.merge(debt, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(bps, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(weeklyReturn, on='end_date',how='left')\n",
    "    totalDfWeekly = totalDfWeekly.merge(eAndSigmaEweekly, on='end_date',how='left')\n",
    "    finalDfWeekly = totalDfWeekly.dropna(axis=0,how='any')\n",
    "    \n",
    "\n",
    "    outputWeekly = []\n",
    "    longthWeekly = finalDfWeekly.shape[0]\n",
    "    compcodeWeekly = []\n",
    "    for ye in range(longthWeekly):\n",
    "        compcodeWeekly.append(i)\n",
    "        EquityWeekly = finalDfWeekly['E'].tolist()[ye]\n",
    "        SigmaEWeekly = finalDfWeekly['sigmaE'].tolist()[ye]\n",
    "        DWeekly = finalDfWeekly['D'].tolist()[ye]\n",
    "        rWeekly = finalDfWeekly['r'].tolist()[ye]/100\n",
    "        mediumWeekly = f(EquityWeekly,SigmaEWeekly,DWeekly,rWeekly,1)\n",
    "        if mediumWeekly[0] == 0 or mediumWeekly[1] == 0:\n",
    "            outputWeekly.append('-')\n",
    "        else:\n",
    "            dd = (mediumWeekly[0] - finalDfWeekly['DP50'].tolist()[ye])/(mediumWeekly[0]*mediumWeekly[1])\n",
    "            outputWeekly.append(1-N(dd))\n",
    "    outputDfWeekly = pd.DataFrame([finalDfWeekly['end_date'].tolist(),outputWeekly]).T\n",
    "    outputDfWeekly.columns = ['end_date','default_probability']\n",
    "    outputDfWeekly.to_excel(i+'_weekly.xlsx', index=False, encoding='utf_8_sig')\n",
    "    \n",
    "    if len(compcodeWeekly) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        togetherWeeklyOutput = pd.DataFrame([compcodeWeekly, finalDfWeekly['end_date'].tolist(), outputWeekly]).T\n",
    "        togetherWeeklyFinalCSMAR_rfmean.append(togetherWeeklyOutput)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "outputTogetherCSMAR_rfmean = togetherFinalCSMAR_rfmean\n",
    "outputWeeklyTogetherCSMAR_rfmean = togetherWeeklyFinalCSMAR_rfmean\n",
    "togetherDfCSMAR_rfmean = pd.concat(outputTogetherCSMAR_rfmean)\n",
    "togetherDfCSMAR_rfmean = togetherDfCSMAR_rfmean.drop_duplicates()\n",
    "togetherDfCSMAR_rfmean.columns = ['compcode','end_date','default_probability']\n",
    "togetherDfCSMAR_rfmean.to_excel('together_final_CSMAR_rfmean.xlsx', index=False, encoding='utf_8_sig')\n",
    "togetherWeeklyDfCSMAR_rfmean = pd.concat(outputWeeklyTogetherCSMAR_rfmean)\n",
    "togetherWeeklyDfCSMAR_rfmean = togetherWeeklyDfCSMAR_rfmean.drop_duplicates()\n",
    "togetherWeeklyDfCSMAR_rfmean.columns = ['compcode','end_date','default_probability']\n",
    "togetherWeeklyDfCSMAR_rfmean.to_excel('together_weekly_return_final_CSMAR_rfmean.xlsx', index=False, encoding='utf_8_sig')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd71f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tushare data result merge\n",
    "framesln_rfmean = []\n",
    "framesWeekly_rfmean = []\n",
    "for i in range(len(defaultCompcode)):\n",
    "    df_ln_rfmean = togetherDf_rfmean[togetherDf_rfmean['compcode'] == defaultCompcode[i]]\n",
    "    df_ln_rfmean['default_date'] = newDef_compdate[i]\n",
    "    framesln_rfmean.append(df_ln_rfmean)\n",
    "    df_weekly_rfmean = togetherWeeklyDf_rfmean[togetherWeeklyDf_rfmean['compcode'] == defaultCompcode[i]]\n",
    "    df_weekly_rfmean['default_date'] = newDef_compdate[i]\n",
    "    framesWeekly_rfmean.append(df_weekly_rfmean)\n",
    "defaultExtractln_rfmean = pd.concat(framesln_rfmean)\n",
    "defaultExtractln_rfmean = defaultExtractln_rfmean.drop_duplicates()\n",
    "defaultExtractWeekly_rfmean = pd.concat(framesWeekly_rfmean)\n",
    "defaultExtractWeekly_rfmean = defaultExtractWeekly_rfmean.drop_duplicates()\n",
    "os.chdir('Default_Data/')\n",
    "defaultExtractln_rfmean.to_excel('default_prob_summary_ln_rfmean.xlsx', index=False, encoding='utf_8_sig')\n",
    "defaultExtractWeekly_rfmean.to_excel('default_prob_summary_weekly_rfmean.xlsx', index=False, encoding='utf_8_sig')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f37f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Result with ln return, rf_mean and Tushare debt data\n",
    "defaultLnMeanTuFrame = []\n",
    "for i in list(set(defaultExtractln_rfmean['compcode'].tolist())):\n",
    "    result = defaultExtractln_rfmean[defaultExtractln_rfmean['compcode'] == i]\n",
    "    if (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231') in result['end_date'].tolist():\n",
    "        defaultLnMeanTuFrame.append(result[result['end_date'] == (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231')])\n",
    "    else:\n",
    "        pass\n",
    "defaultLnMeanTu = pd.concat(defaultLnMeanTuFrame)\n",
    "os.chdir('Default_Data/final_result/')\n",
    "defaultLnMeanTu.to_excel('defaultLnMeanTushare.xlsx', index=False, encoding='sig_utf_8')\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plt.bar(defaultLnMeanTu['compcode'].tolist(), defaultLnMeanTu['default_probability'].tolist())\n",
    "fig.savefig('defaultLnMeanTushare.jpg', dpi = 750)\n",
    "plt.close()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf8ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Result with weekly return, rf_mean and Tushare debt data\n",
    "defaultWeeklyMeanTuFrame = []\n",
    "for i in list(set(defaultExtractWeekly_rfmean['compcode'].tolist())):\n",
    "    result = defaultExtractWeekly_rfmean[defaultExtractWeekly_rfmean['compcode'] == i]\n",
    "    if (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231') in result['end_date'].tolist():\n",
    "        defaultWeeklyMeanTuFrame.append(result[result['end_date'] == (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231')])\n",
    "    else:\n",
    "        pass\n",
    "defaultWeeklyMeanTu = pd.concat(defaultWeeklyMeanTuFrame)\n",
    "os.chdir('Default_Data/final_result/')\n",
    "defaultWeeklyMeanTu.to_excel('defaultWeeklyMeanTushare.xlsx', index=False, encoding='sig_utf_8')\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plt.bar(defaultWeeklyMeanTu['compcode'].tolist(), defaultWeeklyMeanTu['default_probability'].tolist())\n",
    "fig.savefig('defaultWeeklyMeanTushare.jpg', dpi = 750)\n",
    "plt.close()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc99702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d224449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSMAR data result merge\n",
    "frameslnCSMAR_rfmean = []\n",
    "framesWeeklyCSMAR_rfmean = []\n",
    "for i in range(len(defaultCompcode)):\n",
    "    df_ln_CSMAR_rfmean = togetherDfCSMAR_rfmean[togetherDfCSMAR_rfmean['compcode'] == defaultCompcode[i]]\n",
    "    df_ln_CSMAR_rfmean['default_date'] = newDef_compdate[i]\n",
    "    frameslnCSMAR_rfmean.append(df_ln_CSMAR_rfmean)\n",
    "    df_weekly_CSMAR_rfmean = togetherWeeklyDfCSMAR_rfmean[togetherWeeklyDfCSMAR_rfmean['compcode'] == defaultCompcode[i]]\n",
    "    df_weekly_CSMAR_rfmean['default_date'] = newDef_compdate[i]\n",
    "    framesWeeklyCSMAR_rfmean.append(df_weekly_CSMAR_rfmean)\n",
    "defaultExtractlnCSMAR_rfmean = pd.concat(frameslnCSMAR_rfmean)\n",
    "defaultExtractlnCSMAR_rfmean = defaultExtractlnCSMAR_rfmean.drop_duplicates()\n",
    "defaultExtractWeeklyCSMAR_rfmean = pd.concat(framesWeeklyCSMAR_rfmean)\n",
    "defaultExtractWeeklyCSMAR_rfmean = defaultExtractWeeklyCSMAR_rfmean.drop_duplicates()\n",
    "os.chdir('Default_Data/')\n",
    "defaultExtractlnCSMAR_rfmean.to_excel('default_prob_summary_ln_CSMAR_rfmean.xlsx', index=False, encoding='utf_8_sig')\n",
    "defaultExtractWeeklyCSMAR_rfmean.to_excel('default_prob_summary_weekly_CSMAR_rfmean.xlsx', index=False, encoding='utf_8_sig')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Result with ln return, rf_mean and CSMAR debt data\n",
    "defaultLnMeanCSMARFrame = []\n",
    "for i in list(set(defaultExtractlnCSMAR_rfmean['compcode'].tolist())):\n",
    "    result = defaultExtractlnCSMAR_rfmean[defaultExtractlnCSMAR_rfmean['compcode'] == i]\n",
    "    if (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231') in result['end_date'].tolist():\n",
    "        defaultLnMeanCSMARFrame.append(result[result['end_date'] == (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231')])\n",
    "    else:\n",
    "        pass\n",
    "defaultLnMeanCSMAR = pd.concat(defaultLnMeanCSMARFrame)\n",
    "os.chdir('Default_Data/final_result/')\n",
    "defaultLnMeanCSMAR.to_excel('defaultLnMeanCSMAR.xlsx', index=False, encoding='sig_utf_8')\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plt.bar(defaultLnMeanCSMAR['compcode'].tolist(), defaultLnMeanCSMAR['default_probability'].tolist())\n",
    "fig.savefig('defaultLnMeanCSMAR.jpg', dpi = 750)\n",
    "plt.close()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Result with weekly return, rf_mean and CSMAR debt data\n",
    "defaultWeeklyMeanCSMARFrame = []\n",
    "for i in list(set(defaultExtractWeeklyCSMAR_rfmean['compcode'].tolist())):\n",
    "    result = defaultExtractWeeklyCSMAR_rfmean[defaultExtractWeeklyCSMAR_rfmean['compcode'] == i]\n",
    "    if (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231') in result['end_date'].tolist():\n",
    "        defaultWeeklyMeanCSMARFrame.append(result[result['end_date'] == (str(eval(result['default_date'].tolist()[0][:4]) - 1) + '1231')])\n",
    "    else:\n",
    "        pass\n",
    "defaultWeeklyMeanCSMAR = pd.concat(defaultWeeklyMeanCSMARFrame)\n",
    "os.chdir('Default_Data/final_result/')\n",
    "defaultWeeklyMeanCSMAR.to_excel('defaultWeeklyMeanCSMAR.xlsx', index=False, encoding='sig_utf_8')\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plt.bar(defaultWeeklyMeanCSMAR['compcode'].tolist(), defaultWeeklyMeanCSMAR['default_probability'].tolist())\n",
    "fig.savefig('defaultWeeklyMeanCSMAR.jpg', dpi = 750)\n",
    "plt.close()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa95f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "960afac4",
   "metadata": {},
   "source": [
    "# Summary the Tushare and CSMAR, rf_mean, ln_return result to prepare for the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "huizong = []\n",
    "for i in stockCode_SHSZ:\n",
    "    togetherall = pd.DataFrame(lastDate)\n",
    "    togetherall.columns = ['end_date']\n",
    "    togetherall['compcode'] = i\n",
    "    huizong.append(togetherall)\n",
    "resultDataframe = pd.concat(huizong)\n",
    "resultDataframeTushare = resultDataframe\n",
    "resultDataframeCSMAR = resultDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tushare merge\n",
    "tushareMerge = resultDataframeTushare.merge(togetherDf_rfmean, on=['end_date','compcode'], how='left')\n",
    "defaultLastDate = []\n",
    "for i in newDef_compdate:\n",
    "    defaultLastDate.append(str(eval(i[:4])-1)+'1231')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_Date = []\n",
    "for i in tushareMerge['compcode'].tolist():\n",
    "    if i in defaultCompcode:\n",
    "        default_Date.append(defaultLastDate[defaultCompcode.index(i)])\n",
    "    else:\n",
    "        default_Date.append('0')\n",
    "tushareMerge['default_date'] = pd.DataFrame(default_Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_line = []\n",
    "for j in range(tushareMerge.shape[0]):\n",
    "    if (tushareMerge['compcode'].tolist()[j] in defaultCompcode) and (tushareMerge['default_date'].tolist()[j] < tushareMerge['end_date'].tolist()[j]):\n",
    "        delete_line.append(j)\n",
    "    else:\n",
    "        pass\n",
    "tushareMergeFinal = tushareMerge.drop(labels=delete_line, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tushareMergeResult = tushareMergeFinal.reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultJudge = []\n",
    "for i in range(tushareMergeResult.shape[0]):\n",
    "    if tushareMergeResult['default_date'].tolist()[i] == tushareMergeResult['end_date'].tolist()[i]:\n",
    "        defaultJudge.append(1)\n",
    "    else:\n",
    "        defaultJudge.append(0)\n",
    "tushareMergeResult['default_judgment'] = pd.DataFrame(defaultJudge)\n",
    "tushareMergeResult = tushareMergeResult.drop(['default_date'],axis=1)\n",
    "tushareMergeResult.to_excel('tushareMergeFinal.xlsx', index=False, encoding='sig_utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c156a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebbbb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSMAR merge\n",
    "CSMARMerge = resultDataframeCSMAR.merge(togetherDfCSMAR_rfmean, on=['end_date','compcode'], how='left')\n",
    "defaultLastDate = []\n",
    "for i in newDef_compdate:\n",
    "    defaultLastDate.append(str(eval(i[:4])-1)+'1231')\n",
    "default_Date = []\n",
    "for i in CSMARMerge['compcode'].tolist():\n",
    "    if i in defaultCompcode:\n",
    "        default_Date.append(defaultLastDate[defaultCompcode.index(i)])\n",
    "    else:\n",
    "        default_Date.append('0')\n",
    "CSMARMerge['default_date'] = pd.DataFrame(default_Date)\n",
    "delete_line = []\n",
    "for j in range(CSMARMerge.shape[0]):\n",
    "    if (CSMARMerge['compcode'].tolist()[j] in defaultCompcode) and (CSMARMerge['default_date'].tolist()[j] < CSMARMerge['end_date'].tolist()[j]):\n",
    "        delete_line.append(j)\n",
    "    else:\n",
    "        pass\n",
    "CSMARMergeFinal = CSMARMerge.drop(labels=delete_line, axis=0)\n",
    "CSMARMergeResult = CSMARMergeFinal.reset_index().drop(['index'], axis=1)\n",
    "defaultJudge = []\n",
    "for i in range(CSMARMergeResult.shape[0]):\n",
    "    if CSMARMergeResult['default_date'].tolist()[i] == CSMARMergeResult['end_date'].tolist()[i]:\n",
    "        defaultJudge.append(1)\n",
    "    else:\n",
    "        defaultJudge.append(0)\n",
    "CSMARMergeResult['default_judgment'] = pd.DataFrame(defaultJudge)\n",
    "CSMARMergeResult = CSMARMergeResult.drop(['default_date'],axis=1)\n",
    "CSMARMergeResult.to_excel('CSMARMergeFinal.xlsx', index=False, encoding='sig_utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe5d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3b543e2",
   "metadata": {},
   "source": [
    "# Plot the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First input the preparation Tushare data file\n",
    "plotData = tushareMergeResult\n",
    "plotUseData = plotData[['default_probability','default_judgment']]\n",
    "defaultPart = plotUseData[plotUseData['default_judgment'] == 1]\n",
    "notDefaultPart = plotUseData[plotUseData['default_judgment'] == 0]\n",
    "notDefaultPart = notDefaultPart[notDefaultPart['default_probability'] != '-']\n",
    "defaultX = defaultPart['default_probability'].tolist()\n",
    "defaultY = defaultPart['default_judgment'].tolist()\n",
    "notDefaultX = notDefaultPart['default_probability'].tolist()\n",
    "notDefaultY = notDefaultPart['default_judgment'].tolist()\n",
    "# Start to plot with Tushare Data\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plt.scatter(defaultX, defaultY, s=16., color=(0.,0.5,0.))\n",
    "plt.scatter(notDefaultX, notDefaultY, s=16., color=(0.5,0.,0.5))\n",
    "plt.show()\n",
    "#fig.savefig('tusharePlotData.jpg', dpi = 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b1272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Then input the preparation CSMAR data file\n",
    "plotData = CSMARMergeResult\n",
    "plotUseData = plotData[['default_probability','default_judgment']]\n",
    "defaultPart = plotUseData[plotUseData['default_judgment'] == 1]\n",
    "notDefaultPart = plotUseData[plotUseData['default_judgment'] == 0]\n",
    "notDefaultPart = notDefaultPart[notDefaultPart['default_probability'] != '-']\n",
    "defaultX = defaultPart['default_probability'].tolist()\n",
    "defaultY = defaultPart['default_judgment'].tolist()\n",
    "notDefaultX = notDefaultPart['default_probability'].tolist()\n",
    "notDefaultY = notDefaultPart['default_judgment'].tolist()\n",
    "# Start to plot with Tushare Data\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plt.scatter(defaultX, defaultY, s=16., color=(0.,0.5,0.))\n",
    "plt.scatter(notDefaultX, notDefaultY, s=16., color=(0.5,0.,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90343a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First input the preparation Tushare data file\n",
    "plotData = tushareMergeResult\n",
    "plotUseData = plotData[['default_probability','default_judgment']]\n",
    "defaultPart = plotUseData[plotUseData['default_judgment'] == 1]\n",
    "notDefaultPart = plotUseData[plotUseData['default_judgment'] == 0]\n",
    "notDefaultPart = notDefaultPart[notDefaultPart['default_probability'] != '-']\n",
    "defaultX = defaultPart['default_probability'].tolist()\n",
    "defaultY = defaultPart['default_judgment'].tolist()\n",
    "notDefaultX = notDefaultPart['default_probability'].tolist()\n",
    "notDefaultY = notDefaultPart['default_judgment'].tolist()\n",
    "# Start to plot with Tushare Data\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "plt.subplot(211)\n",
    "plt.hist(defaultX, bins=50, color='steelblue')\n",
    "plt.subplot(212)\n",
    "plt.hist(notDefaultX, bins=50, color='red')\n",
    "plt.show()\n",
    "os.chdir('Default_Data/Plot/')\n",
    "fig.savefig('tushareHistogram.jpg', dpi = 750)\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfb211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then input the preparation CSMAR data file\n",
    "plotData = CSMARMergeResult\n",
    "plotUseData = plotData[['default_probability','default_judgment']]\n",
    "defaultPart = plotUseData[plotUseData['default_judgment'] == 1]\n",
    "notDefaultPart = plotUseData[plotUseData['default_judgment'] == 0]\n",
    "notDefaultPart = notDefaultPart[notDefaultPart['default_probability'] != '-']\n",
    "defaultX = defaultPart['default_probability'].tolist()\n",
    "defaultY = defaultPart['default_judgment'].tolist()\n",
    "notDefaultX = notDefaultPart['default_probability'].tolist()\n",
    "notDefaultY = notDefaultPart['default_judgment'].tolist()\n",
    "# Start to plot with Tushare Data\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "plt.subplot(211)\n",
    "plt.hist(defaultX, bins=50, color='steelblue')\n",
    "plt.subplot(212)\n",
    "plt.hist(notDefaultX, bins=50, color='red')\n",
    "plt.show()\n",
    "os.chdir('Default_Data/Plot/')\n",
    "fig.savefig('CSMARHistogram.jpg', dpi = 750)\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab164cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First input the preparation Tushare data file\n",
    "os.chdir('Default_Data/Plot/')\n",
    "plotData = tushareMergeResult\n",
    "plotUseData = plotData[['default_probability','default_judgment']]\n",
    "defaultPart = plotUseData[plotUseData['default_judgment'] == 1]\n",
    "notDefaultPart = plotUseData[plotUseData['default_judgment'] == 0]\n",
    "notDefaultPart = notDefaultPart[notDefaultPart['default_probability'] != '-']\n",
    "defaultX = defaultPart['default_probability'].tolist()\n",
    "defaultY = defaultPart['default_judgment'].tolist()\n",
    "notDefaultX = notDefaultPart['default_probability'].tolist()\n",
    "notDefaultY = notDefaultPart['default_judgment'].tolist()\n",
    "\n",
    "totalDefaultX = defaultX + notDefaultX\n",
    "plotX = [0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95]\n",
    "\n",
    "count_005 = 0\n",
    "count_015 = 0\n",
    "count_025 = 0\n",
    "count_035 = 0\n",
    "count_045 = 0\n",
    "count_055 = 0\n",
    "count_065 = 0\n",
    "count_075 = 0\n",
    "count_085 = 0\n",
    "count_095 = 0\n",
    "for i in totalDefaultX:\n",
    "    if 0 <= i < 0.1:\n",
    "        count_005 += 1\n",
    "    elif 0.1 <= i < 0.2:\n",
    "        count_015 +=1\n",
    "    elif 0.2 <= i < 0.3:\n",
    "        count_025 +=1\n",
    "    elif 0.3 <= i < 0.4:\n",
    "        count_035 +=1\n",
    "    elif 0.4 <= i < 0.5:\n",
    "        count_045 +=1\n",
    "    elif 0.5 <= i < 0.6:\n",
    "        count_055 +=1\n",
    "    elif 0.6 <= i < 0.7:\n",
    "        count_065 +=1\n",
    "    elif 0.7 <= i < 0.8:\n",
    "        count_075 +=1\n",
    "    elif 0.8 <= i <0.9:\n",
    "        count_085 +=1\n",
    "    elif 0.9 <= i <= 1.0:\n",
    "        count_095 +=1\n",
    "plotY = [count_005,count_015,count_025,count_035,count_045,count_055,count_065,count_075,count_085,count_095]\n",
    "\n",
    "count_default_005 = 0\n",
    "count_default_015 = 0\n",
    "count_default_025 = 0\n",
    "count_default_035 = 0\n",
    "count_default_045 = 0\n",
    "count_default_055 = 0\n",
    "count_default_065 = 0\n",
    "count_default_075 = 0\n",
    "count_default_085 = 0\n",
    "count_default_095 = 0\n",
    "for i in defaultX:\n",
    "    if 0 <= i < 0.1:\n",
    "        count_default_005 += 1\n",
    "    elif 0.1 <= i < 0.2:\n",
    "        count_default_015 +=1\n",
    "    elif 0.2 <= i < 0.3:\n",
    "        count_default_025 +=1\n",
    "    elif 0.3 <= i < 0.4:\n",
    "        count_default_035 +=1\n",
    "    elif 0.4 <= i < 0.5:\n",
    "        count_default_045 +=1\n",
    "    elif 0.5 <= i < 0.6:\n",
    "        count_default_055 +=1\n",
    "    elif 0.6 <= i < 0.7:\n",
    "        count_default_065 +=1\n",
    "    elif 0.7 <= i < 0.8:\n",
    "        count_default_075 +=1\n",
    "    elif 0.8 <= i <0.9:\n",
    "        count_default_085 +=1\n",
    "    elif 0.9 <= i <= 1.0:\n",
    "        count_default_095 +=1\n",
    "plotDefaultY = [100*count_default_005/count_005,100*count_default_015/count_015,100*count_default_025/count_025,\n",
    "                100*count_default_035/count_035,100*count_default_045/count_045,100*count_default_055/count_055,\n",
    "                100*count_default_065/count_065,100*count_default_075/count_075,100*count_default_085/count_085,\n",
    "                100*count_default_095/count_095]\n",
    "\n",
    "\n",
    "# Start to plot with Tushare Data\n",
    "fig = plt.figure(figsize=(40,30))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "cmap_names = [\"rainbow\"]\n",
    "for i, name in enumerate(cmap_names):\n",
    "    cmap = mpl.cm.get_cmap(name, len(plotX))\n",
    "    colors = cmap(np.linspace(0,1,len(plotX)))\n",
    "    ax1.bar(plotX, plotY, width=0.05, color=colors, edgecolor='black')\n",
    "\n",
    "ax1.set_ylabel('Number of Issuers',size=50)\n",
    "ax1.set_xlabel('Default probability predictions',size=50)\n",
    "plt.yticks(fontsize=50)\n",
    "plt.xticks(np.arange(0,1,0.1), fontsize=50)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(plotX, plotDefaultY, color='red', label='Default rate')\n",
    "ax2.set_ylabel('Actual default rate (%)',size=50)\n",
    "plt.yticks(np.arange(0,6,1),fontsize=50)\n",
    "plt.title('KMV - Bond Issuer Default Rate Prediction',fontsize=50)\n",
    "plt.legend(loc='best',fontsize=40)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('tushareBar.jpg', dpi = 750)\n",
    "os.chdir('../../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_names = [\"rainbow\"]\n",
    "for i, name in enumerate(cmap_names):\n",
    "    print(i, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe768db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f10193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then input the preparation CSMAR data file\n",
    "os.chdir('Default_Data/Plot/')\n",
    "plotData = CSMARMergeResult\n",
    "plotUseData = plotData[['default_probability','default_judgment']]\n",
    "defaultPart = plotUseData[plotUseData['default_judgment'] == 1]\n",
    "notDefaultPart = plotUseData[plotUseData['default_judgment'] == 0]\n",
    "notDefaultPart = notDefaultPart[notDefaultPart['default_probability'] != '-']\n",
    "defaultX = defaultPart['default_probability'].tolist()\n",
    "defaultY = defaultPart['default_judgment'].tolist()\n",
    "notDefaultX = notDefaultPart['default_probability'].tolist()\n",
    "notDefaultY = notDefaultPart['default_judgment'].tolist()\n",
    "\n",
    "\n",
    "totalDefaultX = defaultX + notDefaultX\n",
    "plotX = [0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95]\n",
    "\n",
    "count_005 = 0\n",
    "count_015 = 0\n",
    "count_025 = 0\n",
    "count_035 = 0\n",
    "count_045 = 0\n",
    "count_055 = 0\n",
    "count_065 = 0\n",
    "count_075 = 0\n",
    "count_085 = 0\n",
    "count_095 = 0\n",
    "for i in totalDefaultX:\n",
    "    if 0 <= i < 0.1:\n",
    "        count_005 += 1\n",
    "    elif 0.1 <= i < 0.2:\n",
    "        count_015 +=1\n",
    "    elif 0.2 <= i < 0.3:\n",
    "        count_025 +=1\n",
    "    elif 0.3 <= i < 0.4:\n",
    "        count_035 +=1\n",
    "    elif 0.4 <= i < 0.5:\n",
    "        count_045 +=1\n",
    "    elif 0.5 <= i < 0.6:\n",
    "        count_055 +=1\n",
    "    elif 0.6 <= i < 0.7:\n",
    "        count_065 +=1\n",
    "    elif 0.7 <= i < 0.8:\n",
    "        count_075 +=1\n",
    "    elif 0.8 <= i <0.9:\n",
    "        count_085 +=1\n",
    "    elif 0.9 <= i <= 1.0:\n",
    "        count_095 +=1\n",
    "plotY = [count_005,count_015,count_025,count_035,count_045,count_055,count_065,count_075,count_085,count_095]\n",
    "\n",
    "count_default_005 = 0\n",
    "count_default_015 = 0\n",
    "count_default_025 = 0\n",
    "count_default_035 = 0\n",
    "count_default_045 = 0\n",
    "count_default_055 = 0\n",
    "count_default_065 = 0\n",
    "count_default_075 = 0\n",
    "count_default_085 = 0\n",
    "count_default_095 = 0\n",
    "for i in defaultX:\n",
    "    if 0 <= i < 0.1:\n",
    "        count_default_005 += 1\n",
    "    elif 0.1 <= i < 0.2:\n",
    "        count_default_015 +=1\n",
    "    elif 0.2 <= i < 0.3:\n",
    "        count_default_025 +=1\n",
    "    elif 0.3 <= i < 0.4:\n",
    "        count_default_035 +=1\n",
    "    elif 0.4 <= i < 0.5:\n",
    "        count_default_045 +=1\n",
    "    elif 0.5 <= i < 0.6:\n",
    "        count_default_055 +=1\n",
    "    elif 0.6 <= i < 0.7:\n",
    "        count_default_065 +=1\n",
    "    elif 0.7 <= i < 0.8:\n",
    "        count_default_075 +=1\n",
    "    elif 0.8 <= i <0.9:\n",
    "        count_default_085 +=1\n",
    "    elif 0.9 <= i <= 1.0:\n",
    "        count_default_095 +=1\n",
    "plotDefaultY = [count_default_005/count_005,count_default_015/count_015,count_default_025/count_025,\n",
    "                count_default_035/count_035,count_default_045/count_045,count_default_055/count_055,\n",
    "                count_default_065/count_065,count_default_075/count_075,count_default_085/count_085,\n",
    "                count_default_095/count_095]\n",
    "\n",
    "\n",
    "# Start to plot with Tushare Data\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.bar(plotX, plotY, width=0.05, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Issuers',size=20)\n",
    "ax1.set_xlabel('Default probability predictions',size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(np.arange(0,1,0.1), fontsize=20)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(plotX, plotDefaultY, color='red', label='Default rate')\n",
    "ax2.set_ylabel('Actual default rate (%)',size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title('KMV - Bond Issuer Default Rate Prediction',fontsize=20)\n",
    "plt.legend(loc='best',fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('CSMARBar.jpg', dpi = 750)\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5eb0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb51e44c",
   "metadata": {},
   "source": [
    "# ROC and AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99080a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set cutoff values\n",
    "cutoff = np.arange(0,1.001,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b48d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "tushareMergeResult = tushareMergeResult[tushareMergeResult['default_probability'] != '-']\n",
    "afterDeleteNa = tushareMergeResult.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496c10b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Then, build Confusion Matrix\n",
    "tpr = []\n",
    "fpr = []\n",
    "for i in cutoff:\n",
    "    tp = afterDeleteNa.loc[(afterDeleteNa['default_probability'] >= i) & (afterDeleteNa['default_judgment'] == 1)]\n",
    "    fn = afterDeleteNa.loc[(afterDeleteNa['default_probability'] < i) & (afterDeleteNa['default_judgment'] == 1)]\n",
    "    fp = afterDeleteNa.loc[(afterDeleteNa['default_probability'] >= i) & (afterDeleteNa['default_judgment'] == 0)]\n",
    "    tn = afterDeleteNa.loc[(afterDeleteNa['default_probability'] < i) & (afterDeleteNa['default_judgment'] == 0)]    \n",
    "    \n",
    "    tpr.append(tp.shape[0]/(tp.shape[0]+fn.shape[0]))\n",
    "    fpr.append(fp.shape[0]/(fp.shape[0]+tn.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4836d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then plot ROC\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "plt.plot(fpr,tpr,linestyle='-',color='green', label='Predicted ROC Curve',linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\", label=\"Random ROC Curve\",linewidth=2)\n",
    "plt.xlabel('False Positive Rate',fontsize=20)\n",
    "plt.ylabel('True Positive Rate',fontsize=20)\n",
    "plt.yticks(np.arange(0,1.1,0.1), fontsize=20)\n",
    "plt.xticks(np.arange(0,1.1,0.1), fontsize=20)\n",
    "# Ideal Curve\n",
    "best_point = [afterDeleteNa[afterDeleteNa['default_judgment'] == 1].shape[0] / afterDeleteNa.shape[0], 1]\n",
    "plt.plot([0, best_point[0], 1], [0, best_point[1], 1], color=\"red\", label=\"Ideal ROC Curve\", zorder=10,linewidth=2)\n",
    "# Fill in Color\n",
    "plt.fill_between(fpr, tpr, fpr, color=\"yellow\", alpha=0.3)\n",
    "plt.fill_between(fpr, [1 if i * afterDeleteNa.shape[0] / afterDeleteNa[afterDeleteNa['default_judgment'] == 1].shape[0] >= 1 else i * afterDeleteNa.shape[0] / afterDeleteNa[afterDeleteNa['default_judgment'] == 1].shape[0] for i in fpr], tpr, color=\"gray\", alpha=0.3)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic Curve',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "os.chdir('Default_Data/Plot/')\n",
    "fig.savefig('ROC.jpg', dpi = 750)\n",
    "os.chdir('../../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUROC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "afterDeleteNa = tushareMergeResult.dropna(axis=0,how='any')\n",
    "y_truth = np.array(afterDeleteNa['default_judgment'].tolist())\n",
    "y_pred = np.array(afterDeleteNa['default_probability'].tolist())\n",
    "auc_score = roc_auc_score(y_truth,y_pred)\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f1e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aed54e2e",
   "metadata": {},
   "source": [
    "# CAP and AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b678d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also set cutoff\n",
    "cutoff = np.arange(0,1.001,0.001)\n",
    "tushareMergeResult = tushareMergeResult[tushareMergeResult['default_probability'] != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd62dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = []\n",
    "cdr = []\n",
    "gini_together_1 = []\n",
    "gini_together_2 = []\n",
    "for i in cutoff:\n",
    "    tr = afterDeleteNa.loc[afterDeleteNa['default_probability'] >= i]\n",
    "    dr = afterDeleteNa.loc[(afterDeleteNa['default_probability'] >= i) & (afterDeleteNa['default_judgment'] == 1)]\n",
    "    dt = afterDeleteNa.loc[afterDeleteNa['default_judgment'] == 1]\n",
    "    ctr.append(tr.shape[0]/afterDeleteNa.shape[0])\n",
    "    cdr.append(dr.shape[0]/dt.shape[0])\n",
    "    gini_together_1.append([tr.shape[0]/afterDeleteNa.shape[0], dr.shape[0]/dt.shape[0]])\n",
    "    gini_together_2.append([dr.shape[0]/dt.shape[0], tr.shape[0]/afterDeleteNa.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2becbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_score = 2*auc_score - 1\n",
    "ar_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gini(data_list):\n",
    "    data_length = len(data_list)\n",
    "    total_sum = np.sum(data_list)\n",
    "    total_gini = 0\n",
    "    for i in range(data_length):\n",
    "        temp_denominator = data_list[i][0] + data_list[i][1]\n",
    "        temp_son = np.sum(data_list[i])\n",
    "        temp_gini = 1 - np.power(data_list[i][0]/temp_denominator, 2) - np.power(data_list[i][1]/temp_denominator, 2)\n",
    "        total_gini = temp_gini * temp_son / total_sum + total_gini\n",
    "    return total_gini\n",
    "print(gini(gini_together_1))\n",
    "print(gini(gini_together_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "\n",
    "def cap_plot(predictions, labels, cut_point=100000):\n",
    "    sample_size = len(labels)\n",
    "    bad_label_size = len([i for i in labels if i == 1])\n",
    "    score_thres = np.linspace(1, 0, cut_point)\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for thres in score_thres:\n",
    "        x = len([i for i in predictions if i >= thres])\n",
    "        x_list.append(x / sample_size)\n",
    "        y = len([(i, j) for i, j in zip(predictions, labels) if i >= thres and j == 1])\n",
    "        y_list.append(y / bad_label_size)\n",
    "\n",
    "    # Real Curve\n",
    "    fig = plt.figure(figsize=(30,20))\n",
    "    plt.plot(x_list, y_list, color=\"green\", label=\"Predicted Curve\",linewidth=2)\n",
    "    plt.yticks(np.arange(0,1.1,0.1),fontsize=20)\n",
    "    plt.xticks(np.arange(0,1.1,0.1),fontsize=20)\n",
    "    \n",
    "    # Ideal Curve\n",
    "    best_point = [bad_label_size / sample_size, 1]\n",
    "    plt.plot([0, best_point[0], 1], [0, best_point[1], 1], color=\"red\", label=\"Ideal Curve\", zorder=10,linewidth=2)\n",
    "    # Add the ideal point's axes\n",
    "    #plt.scatter(best_point[0], 1, color=\"white\", edgecolors=\"red\", s=30, zorder=30)\n",
    "    #plt.text(best_point[0] + 0.1, 0.95, \"{}/{},{}\".format(bad_label_size, sample_size, 1), ha=\"center\", va=\"center\")\n",
    "    \n",
    "    # Random Curve\n",
    "    plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\", label=\"Random Curve\",linewidth=2)\n",
    "    \n",
    "    # Fill in Color\n",
    "    plt.fill_between(x_list, y_list, x_list, color=\"yellow\", alpha=0.3)\n",
    "    plt.fill_between(x_list, [1 if i * sample_size / bad_label_size >= 1 else i * sample_size / bad_label_size for i in x_list], y_list, color=\"gray\", alpha=0.3)\n",
    "\n",
    "    # Calculate AR Value\n",
    "    # Square under Real Curve\n",
    "    actual_area = np.trapz(y_list, x_list) - 1 * 1 / 2\n",
    "    best_area = 1 * 1 / 2 - 1 * bad_label_size / sample_size / 2\n",
    "    ar_value = actual_area / best_area\n",
    "    plt.title(\"Cumulative Accuracy Profile Curve (AR={:.4f})\".format(ar_value), fontsize=20)\n",
    "    plt.xlabel('Cumulative proportion of real default in total universe', fontsize=20)\n",
    "    plt.ylabel('Cumulative proportion of real default in default universe', fontsize=20)\n",
    "\n",
    "    plt.legend(loc=4, fontsize=20)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    os.chdir('Default_Data/Plot/')\n",
    "    fig.savefig('CAP.jpg', dpi = 750)\n",
    "    os.chdir('../../')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cap_plot(afterDeleteNa['default_probability'], afterDeleteNa['default_judgment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e6c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f813f568",
   "metadata": {},
   "source": [
    "# K-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c51232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, build Confusion Matrix\n",
    "cutoff = np.arange(0,1.001,0.001)\n",
    "tpr = []\n",
    "fpr = []\n",
    "for i in cutoff:\n",
    "    tp = tushareMergeResult.loc[(tushareMergeResult['default_probability'] >= i) & (tushareMergeResult['default_judgment'] == 1)]\n",
    "    fn = tushareMergeResult.loc[(tushareMergeResult['default_probability'] < i) & (tushareMergeResult['default_judgment'] == 1)]\n",
    "    fp = tushareMergeResult.loc[(tushareMergeResult['default_probability'] >= i) & (tushareMergeResult['default_judgment'] == 0)]\n",
    "    tn = tushareMergeResult.loc[(tushareMergeResult['default_probability'] < i) & (tushareMergeResult['default_judgment'] == 0)]    \n",
    "    \n",
    "    tpr.append(tp.shape[0]/(tp.shape[0]+fn.shape[0]))\n",
    "    fpr.append(fp.shape[0]/(fp.shape[0]+tn.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then plot K-S\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "ks = np.array(tpr) - np.array(fpr)\n",
    "print(ks.max())\n",
    "plt.plot(cutoff,tpr,linestyle='-',color='r', label='TPR',linewidth=2)\n",
    "plt.plot(cutoff,fpr,linestyle='-',color='green', label='FPR',linewidth=2)\n",
    "plt.vlines(cutoff[list(ks).index(ks.max())], fpr[list(ks).index(ks.max())], tpr[list(ks).index(ks.max())], color='blue', linestyle='--',linewidth=2)\n",
    "plt.xlabel('Cutoff',fontsize=20)\n",
    "plt.ylabel('Percent population',fontsize=20)\n",
    "\n",
    "plt.yticks(np.arange(0,1.1,0.1), fontsize=20)\n",
    "plt.xticks(np.arange(0,1.1,0.1), fontsize=20)\n",
    "plt.title('K-S Curve',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "os.chdir('Default_Data/Plot/')\n",
    "fig.savefig('KS.jpg', dpi = 750)\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.stats import ks_2samp\n",
    "def ks_calc_cross(data,pred,y_label):\n",
    "    crossfreq = pd.crosstab(data[pred[0]],data[y_label[0]])\n",
    "    crossdens = crossfreq.cumsum(axis=0) / crossfreq.sum()\n",
    "    crossdens['gap'] = abs(crossdens[0] - crossdens[1])\n",
    "    ks = crossdens[crossdens['gap'] == crossdens['gap'].max()]\n",
    "    return ks,crossdens\n",
    "\n",
    "def ks_calc_auc(data,pred,y_label):\n",
    "    fpr,tpr,thresholds= roc_curve(data[y_label[0]],data[pred[0]])\n",
    "    ks = max(tpr-fpr)\n",
    "    return ks\n",
    "\n",
    "def ks_calc_2samp(data,pred,y_label):\n",
    "    Bad = data.loc[data[y_label[0]]==1,pred[0]]\n",
    "    Good = data.loc[data[y_label[0]]==0, pred[0]]\n",
    "    data1 = Bad.values\n",
    "    data2 = Good.values\n",
    "    n1 = data1.shape[0]\n",
    "    n2 = data2.shape[0]\n",
    "    data1 = np.sort(data1)\n",
    "    data2 = np.sort(data2)\n",
    "    data_all = np.concatenate([data1,data2])\n",
    "    cdf1 = np.searchsorted(data1,data_all,side='right')/(1.0*n1)\n",
    "    cdf2 = (np.searchsorted(data2,data_all,side='right'))/(1.0*n2)\n",
    "    ks = np.max(np.absolute(cdf1-cdf2))\n",
    "    cdf1_df = pd.DataFrame(cdf1)\n",
    "    cdf2_df = pd.DataFrame(cdf2)\n",
    "    cdf_df = pd.concat([cdf1_df,cdf2_df],axis = 1)\n",
    "    cdf_df.columns = ['cdf_Bad','cdf_Good']\n",
    "    cdf_df['gap'] = cdf_df['cdf_Bad']-cdf_df['cdf_Good']\n",
    "    return ks,cdf_df\n",
    "\n",
    "data = afterDeleteNa[['default_probability','default_judgment']]\n",
    "ks1,crossdens=ks_calc_cross(data,['default_probability'], ['default_judgment'])\n",
    "ks2=ks_calc_auc(data,['default_probability'], ['default_judgment'])\n",
    "ks3=ks_calc_2samp(data,['default_probability'], ['default_judgment'])\n",
    "get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statistic\n",
    "ks4=get_ks(data['default_probability'],data['default_judgment'])\n",
    "print('KS1:',ks1['gap'].values)\n",
    "print('KS2:',ks2)\n",
    "print('KS3:',ks3[0])\n",
    "print('KS4:',ks4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6be04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e7adacc2dea369be92f26dedf2faca9612150f7403929a4310f5fe00c263e61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
